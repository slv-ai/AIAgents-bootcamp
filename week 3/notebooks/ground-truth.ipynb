{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8edb6e4d-0693-4205-a3e4-234b5edacd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1dae5c-e3f5-49fa-b923-197f04471e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5973034a-f3e7-4b8f-b5d7-bd7ebf0dfdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data definition 11\n",
      "descriptors 12\n",
      "overview 3\n",
      "metric generators 2\n",
      "output formats 1\n",
      "introduction 22\n",
      "report 4\n",
      "add tags and metadata 2\n",
      "tests 9\n",
      "alerts 1\n",
      "add dashboard panels (api) 13\n",
      "add dashboard panels (ui) 4\n",
      "overview 2\n",
      "overview 2\n",
      "work with datasets 2\n",
      "run evals via api 2\n",
      "explore view 1\n",
      "no code evals 4\n",
      "overview 2\n",
      "batch monitoring 2\n",
      "overview 3\n",
      "introduction 2\n",
      "manage projects 4\n",
      "overview 1\n",
      "overview 1\n",
      "set up tracing 10\n",
      "evidently cloud 1\n",
      "self-hosting 5\n",
      "evidently and github actions 1\n",
      "llm evaluations 2\n",
      "llm as a judge 21\n",
      "llm-as-a-jury 9\n",
      "rag evals 13\n",
      "llm regression testing 21\n",
      "tutorials and guides 12\n",
      "evidently cloud v2 1\n",
      "migration guide 7\n",
      "open-source vs. cloud 6\n",
      "telemetry 10\n",
      "why evidently? 4\n",
      "what is evidently? 1\n",
      "all descriptors 31\n",
      "all metrics 54\n",
      "overview 1\n",
      "customize data drift 17\n",
      "custom text descriptor 3\n",
      "use huggingface models 10\n",
      "configure llm judges 26\n",
      "custom metric 4\n",
      "classification metrics 8\n",
      "data stats and quality 7\n",
      "data drift 8\n",
      "ranking and recsys metrics 10\n",
      "regression metrics 9\n",
      "classification 3\n",
      "data drift 5\n",
      "data summary 3\n",
      "recommendations 3\n",
      "regression 3\n",
      "text evals 3\n",
      "llm evaluation 9\n",
      "data and ml checks 6\n",
      "tracing 5\n",
      "adversarial testing 2\n",
      "create synthetic inputs 1\n",
      "synthetic data 1\n",
      "rag evaluation dataset 1\n",
      "why synthetic data? 2\n"
     ]
    }
   ],
   "source": [
    "selected_docs = []\n",
    "total_questions = 0\n",
    "\n",
    "for doc in documents:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title'].lower()\n",
    "\n",
    "    content = doc.get('content', '')\n",
    "\n",
    "    if len(content) < 1000:\n",
    "        continue\n",
    "    \n",
    "    if 'unpublished' in title:\n",
    "        continue\n",
    "\n",
    "    if 'legacy' in title:\n",
    "        continue\n",
    "\n",
    "    if 'leftovers' in title:\n",
    "        continue\n",
    "\n",
    "    if 'updates' in title:\n",
    "        continue\n",
    "\n",
    "    num_questions = len(content) // 1000\n",
    "    total_questions = total_questions + num_questions\n",
    "    print(title, num_questions)\n",
    "    selected_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46369cc2-a833-4c67-a749-c022b0cb0b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e9a18e-3e48-465f-863f-48ca793ae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return (response.output_parsed, response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586e188d-76d3-407f-b1c3-ad8c08edabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7133f8b-1ca4-48eb-9ae4-6157de4c2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b43382d-df82-4b07-990e-14d5b5d087d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d34cca0f-c4da-4a70-be7e-543719750b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    generate {num_questions} questions for this document:\n",
    "{json.dumps(doc)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    output, usage = llm_structured(\n",
    "        instructions=instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions,\n",
    "    )\n",
    "   \n",
    "    return {'doc': doc, 'questions': output, 'usage': usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebdfec98-5ad0-47cf-b98a-50f5223a4541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 68/68 [01:34<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    results = map_progress(pool, selected_docs, process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0d5fd9a-0eca-4649-9678-0e0537681d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca5d6fa1-ae0e-46e5-9c4f-e818c65f4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    doc = r['doc']\n",
    "    questions = r['questions']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ccee07-749e-4983-b2d3-1867994aed15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratedQuestions(description='This article explains how to map input data using the `DataDefinition` object in Evidently. It details the different column types, roles, and mapping methods to ensure accurate data evaluations.', questions=[Question(question='mapping input data in Evidently', summary_answer='The article details how to use the `DataDefinition` object to map input data correctly for data evaluations in Evidently.', difficulty='beginner', intent='text'), Question(question='define column types in DataDefinition', summary_answer='It outlines the various column types that can be defined in a `DataDefinition`, including categorical, numerical, text, and datetime columns.', difficulty='beginner', intent='text'), Question(question='create Dataset object in Evidently', summary_answer='To create a `Dataset` object, you can use `Dataset.from_pandas` with a specified `DataDefinition` to ensure correct data processing.', difficulty='beginner', intent='code'), Question(question='DataDefinition manual mapping examples', summary_answer='The article provides examples of how to manually define a `DataDefinition`, including specific columns for text, numerical, and categorical data.', difficulty='intermediate', intent='code'), Question(question='default column mappings in Evidently', summary_answer='It describes how automatic mapping works for different column types if no explicit mapping is given, detailing the defaults for numerical, categorical, and datetime columns.', difficulty='intermediate', intent='text'), Question(question='pandas.DataFrame to Dataset object', summary_answer=\"You can directly use a `pandas.DataFrame` with the `report.run()` method, but it's recommended to create a `Dataset` object for clarity.\", difficulty='intermediate', intent='code'), Question(question='importance of ID and timestamp columns', summary_answer='The article highlights the significance of correctly identifying ID and timestamp columns for accurate evaluations and their respective roles.', difficulty='beginner', intent='text'), Question(question='multiple regression mapping in DataDefinition', summary_answer='It shows how to set up regression checks by mapping target and prediction columns within the `DataDefinition` for regression scenarios.', difficulty='advanced', intent='code'), Question(question='mapping for LLM evaluations', summary_answer='For LLM evaluations, you can specify text columns in the `DataDefinition` to ensure proper assessment of text data.', difficulty='intermediate', intent='code'), Question(question='binary vs multiclass classification mapping', summary_answer='The article discusses the differences in mapping for binary versus multiclass classification scenarios within the `DataDefinition` framework.', difficulty='advanced', intent='text'), Question(question='automated vs manual column mapping', summary_answer='It explains the pros and cons of automated column mapping versus manual mapping for ensuring accurate data evaluations in Evidently.', difficulty='intermediate', intent='text')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "576c0f21-813c-4c20-9fd1-63d29952e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_questions = []\n",
    "\n",
    "for r in results:\n",
    "    doc = r['doc']\n",
    "    questions = r['questions']\n",
    "\n",
    "    for q in questions.questions:\n",
    "        final_question = q.model_dump()\n",
    "        final_question['filename'] = doc['filename']\n",
    "        final_questions.append(final_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f1c498a-a738-462b-be4c-3150685bdeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'mapping input data in Evidently',\n",
       "  'summary_answer': 'The article details how to use the `DataDefinition` object to map input data correctly for data evaluations in Evidently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'define column types in DataDefinition',\n",
       "  'summary_answer': 'It outlines the various column types that can be defined in a `DataDefinition`, including categorical, numerical, text, and datetime columns.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'create Dataset object in Evidently',\n",
       "  'summary_answer': 'To create a `Dataset` object, you can use `Dataset.from_pandas` with a specified `DataDefinition` to ensure correct data processing.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'DataDefinition manual mapping examples',\n",
       "  'summary_answer': 'The article provides examples of how to manually define a `DataDefinition`, including specific columns for text, numerical, and categorical data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'default column mappings in Evidently',\n",
       "  'summary_answer': 'It describes how automatic mapping works for different column types if no explicit mapping is given, detailing the defaults for numerical, categorical, and datetime columns.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'pandas.DataFrame to Dataset object',\n",
       "  'summary_answer': \"You can directly use a `pandas.DataFrame` with the `report.run()` method, but it's recommended to create a `Dataset` object for clarity.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'importance of ID and timestamp columns',\n",
       "  'summary_answer': 'The article highlights the significance of correctly identifying ID and timestamp columns for accurate evaluations and their respective roles.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'multiple regression mapping in DataDefinition',\n",
       "  'summary_answer': 'It shows how to set up regression checks by mapping target and prediction columns within the `DataDefinition` for regression scenarios.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'mapping for LLM evaluations',\n",
       "  'summary_answer': 'For LLM evaluations, you can specify text columns in the `DataDefinition` to ensure proper assessment of text data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'binary vs multiclass classification mapping',\n",
       "  'summary_answer': 'The article discusses the differences in mapping for binary versus multiclass classification scenarios within the `DataDefinition` framework.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'automated vs manual column mapping',\n",
       "  'summary_answer': 'It explains the pros and cons of automated column mapping versus manual mapping for ensuring accurate data evaluations in Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/data_definition.mdx'},\n",
       " {'question': 'how to evaluate text data with descriptors',\n",
       "  'summary_answer': 'Descriptors are used to compute scores or labels for text data evaluations, allowing for both built-in and custom evaluations across various text statistics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'using python to create descriptors',\n",
       "  'summary_answer': 'The article provides step-by-step Python code to create and use descriptors within a Dataset for evaluating text data.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'custom descriptors in text evaluation',\n",
       "  'summary_answer': 'You can create custom descriptors using LLM prompts or Python functions to tailor evaluations to specific needs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'example of descriptors for sentiment evaluation',\n",
       "  'summary_answer': 'The text includes examples that define sentiment descriptors for evaluating responses in a dataset, showing how to check for sentiments in responses.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'available built-in descriptors',\n",
       "  'summary_answer': 'The article lists several built-in descriptors like Sentiment, TextLength, and IncludesWords for evaluating various aspects of text data.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'how to combine multiple descriptors',\n",
       "  'summary_answer': 'You can combine multiple descriptors in a single Dataset by passing them as a list to enhance evaluation results across different text features.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'Python imports for descriptors',\n",
       "  'summary_answer': \"The guide outlines necessary imports from Evidently for implementing descriptors in text evaluations, ensuring proper utilization of the library's functionalities.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'checking conditions with descriptor tests',\n",
       "  'summary_answer': 'Descriptor Tests allow defining pass/fail checks for evaluations, enabling the assessment of conditions like text length or sentiment positivity for each row.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'generate toy data for testing descriptors',\n",
       "  'summary_answer': 'The article provides a code snippet to create sample data for testing the evaluation process using descriptors in Python.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'exporting results from descriptors',\n",
       "  'summary_answer': 'You can preview and export the results of evaluations as a DataFrame or in various formats such as HTML and JSON for further analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'advanced testing in dataset-level reports',\n",
       "  'summary_answer': 'The article discusses integrating tests into dataset-level metrics to generate clear pass/fail results for comprehensive evaluations of text characteristics.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'configuring text evaluation reports',\n",
       "  'summary_answer': 'Custom reports can be tailored to use specific descriptors or metrics for more focused evaluation results in text datasets.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/descriptors.mdx'},\n",
       " {'question': 'evidently evaluation workflow steps',\n",
       "  'summary_answer': 'The article outlines the core steps of the evaluation workflow with Evidently, including data preparation, creating a Dataset object, and running evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'how to create a Dataset in evidently',\n",
       "  'summary_answer': 'To create a Dataset in Evidently, you use the `Dataset.from_pandas()` method along with a `DataDefinition()` specifying column roles and types.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'adding metadata to evidently evaluation results',\n",
       "  'summary_answer': 'You can enhance your evaluation results in Evidently by adding tags or metadata to identify specific runs, which helps in organizing evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/evaluations_overview.mdx'},\n",
       " {'question': 'generate multiple metrics at once',\n",
       "  'summary_answer': 'The article explains how to use metric generator helper functions to generate multiple metrics for dataset columns in a streamlined manner.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/metric_generator.mdx'},\n",
       " {'question': 'ColumnMetricGenerator example code',\n",
       "  'summary_answer': 'It provides examples of using the ColumnMetricGenerator to apply metrics like ValueDrift to columns in datasets with Python code snippets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/metric_generator.mdx'},\n",
       " {'question': 'exporting reports in JSON format',\n",
       "  'summary_answer': 'The article explains how to export evaluation results as JSON using the `my_report.json()` method, allowing easy storage and export of results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/output_formats.mdx'},\n",
       " {'question': 'Evidently library features',\n",
       "  'summary_answer': 'The Evidently library offers various features for AI/ML evaluations, synthetic data generation, prompt optimization, and a visualization UI to track evaluations over time.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'AI/ML evaluations in Evidently',\n",
       "  'summary_answer': 'Evidently provides over 100 built-in metrics and checks to run evaluations on AI systems, offering results in formats like JSON, DataFrames, and visual reports.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Generate visual reports Evidently',\n",
       "  'summary_answer': 'You can generate visual reports in formats compatible with Jupyter, Colab, or as HTML files, which display multiple metrics and evaluation results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Synthetic data generation Evidently',\n",
       "  'summary_answer': 'Evidently helps generate structured synthetic data primarily designed for LLM use cases, allowing for the creation of test datasets for AI applications.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Prompt optimization tools',\n",
       "  'summary_answer': 'The Evidently library includes tools for optimizing prompts using evaluation capabilities, which can help users generate better prompts based on feedback and targets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Setup tracking and visualization UI',\n",
       "  'summary_answer': 'Evidently offers a lightweight UI for tracking and visualizing evaluation results over time, allowing users to store and compare evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Export metrics and reports',\n",
       "  'summary_answer': 'You can export evaluation scores as JSON, pandas DataFrames, or generate visual reports, facilitating integration into various workflows.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Descriptors in Evidently library',\n",
       "  'summary_answer': 'Descriptors provide row-level assessments for specific qualities of text, enabling detailed evaluations of outputs from AI systems.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Creating a Dataset object in Evidently',\n",
       "  'summary_answer': 'To run evaluations, you must create a Dataset object in Evidently, which allows you to map data columns and attach meta-information for processing.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Evaluating classification quality',\n",
       "  'summary_answer': 'Evidently requires specific columns for evaluating classification quality, mapping actual and predicted labels for insightful comparisons.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Data drift detection methods',\n",
       "  'summary_answer': 'Evidently includes over 20 tests and metrics to detect data drift by comparing current datasets with reference datasets to identify distribution changes.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Reports and metrics functionality',\n",
       "  'summary_answer': 'Reports in Evidently allow users to summarize and analyze datasets, running evaluations across various metrics and providing visual summaries.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Conditional tests in Evidently',\n",
       "  'summary_answer': 'Tests in Evidently validate results against specific expectations, allowing users to set conditions for metrics to ensure quality and performance.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Using metric presets',\n",
       "  'summary_answer': 'Evidently offers metric presets that are pre-configured templates for specific evaluation scenarios, streamlining the process of running evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Benefits of using Test Suites',\n",
       "  'summary_answer': 'Test Suites in Evidently allow users to run multiple tests simultaneously, offering a summary of outcomes for effective monitoring and debugging.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Automating evaluations with Evidently',\n",
       "  'summary_answer': \"You can integrate Evidently's evaluation capabilities into automation workflows, triggering actions based on test results within your data pipeline.\",\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Edge cases in dataset evaluations',\n",
       "  'summary_answer': 'Advanced users can explore various edge cases when evaluating datasets in Evidently by customizing tests and metrics according to specific scenarios.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Comparing datasets in evaluations',\n",
       "  'summary_answer': 'Evidently allows comparisons between two datasets, which is useful for side-by-side evaluations or detecting data drift over time.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'Handling large datasets in Evidently',\n",
       "  'summary_answer': 'When working with large datasets, it is advisable to apply sampling methods to optimize performance during evaluations in the Evidently library.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/overview.mdx'},\n",
       " {'question': 'generate reports with Evidently',\n",
       "  'summary_answer': 'This article explains how to use the Evidently library to generate reports, including step-by-step instructions and code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'Evidently report presets',\n",
       "  'summary_answer': 'The article discusses the use of Metric Presets in Evidently to create reports that are ready to use out of the box, along with examples of generating specific reports.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'Data Summary Report example',\n",
       "  'summary_answer': 'An example of generating a Data Summary Report using Evidently is provided, demonstrating how to create and run a report on a single dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'compare results in Evidently reports',\n",
       "  'summary_answer': 'The article details how to compare multiple report snapshots side-by-side using the compare function in Evidently, aiding in result analysis.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'question': 'how to add metadata to Evidently reports',\n",
       "  'summary_answer': 'The article explains how to incorporate metadata into reports by using Python dictionaries for key:value pairs, enhancing report categorization and filtering capabilities.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tags_metadata.mdx'},\n",
       " {'question': 'custom tags and metadata examples Evidently',\n",
       "  'summary_answer': 'The article includes code examples showing how to create custom tags and metadata for reports, allowing users to tailor their evaluations based on specific criteria or contexts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tags_metadata.mdx'},\n",
       " {'question': 'how to import testing modules evidently',\n",
       "  'summary_answer': 'To use Tests, import the following modules: from evidently import Report, from evidently.metrics import *, from evidently.presets import *, from evidently.tests import *.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'types of tests in evidently',\n",
       "  'summary_answer': 'The article discusses three ways to implement Tests: Tests Presets, Tests with defaults, and Custom Tests, allowing for flexible data validation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'example of using test presets evidently',\n",
       "  'summary_answer': 'You can enable Test Presets by setting include_tests=True in the Report, which automatically generates a suite of Tests for your data evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'custom test conditions examples evidently',\n",
       "  'summary_answer': 'You can define specific pass/fail conditions for each Test by using parameters like gt, lt, eq while creating a Report instance.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'using reference dataset for tests evidently',\n",
       "  'summary_answer': 'When using a reference dataset, Tests will compare new data against it to validate conditions, such as checking for missing values.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'difference between include_tests True and False',\n",
       "  'summary_answer': 'Setting include_tests=True adds Tests to your Report while False excludes them, allowing flexibility in what validations to include.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'how to run personalized tests evidently',\n",
       "  'summary_answer': 'You can run personalized tests by selecting specific Metrics and setting conditions manually, giving you control over the validation process.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'setting warning vs fail for tests evidently',\n",
       "  'summary_answer': 'You can configure Tests to return a Warning instead of a Fail by setting is_critical=False, helping to manage alert fatigue.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'conditions for testing share vs absolute evidently',\n",
       "  'summary_answer': 'The article explains how to set conditions for testing both share and absolute values, using parameters like tests for absolute and share_tests for relative comparisons.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/library/tests.mdx'},\n",
       " {'question': 'set up alerts in Evidently',\n",
       "  'summary_answer': 'The article provides a step-by-step guide on setting up alerts in Evidently, including choosing notification channels like Email or Slack and defining alert conditions based on test failures or custom metric values.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/alerts.mdx'},\n",
       " {'question': 'add panel to dashboard api example',\n",
       "  'summary_answer': 'The article provides code examples on how to add panels to a dashboard using the Python API, including multiple types of panels.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard management api',\n",
       "  'summary_answer': 'It details how to manage dashboards programmatically, including adding and deleting tabs and panels using the Python API.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'evidently dashboard panels',\n",
       "  'summary_answer': \"The article explains how to utilize Evidently's API to create and customize dashboard panels for reporting purposes.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'create new tab in dashboard api',\n",
       "  'summary_answer': 'The article specifies how to use the API to create new tabs in a dashboard with the example code provided for `add_tab`.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'delete panel from dashboard',\n",
       "  'summary_answer': 'Code snippets are provided to delete specific panels from the dashboard, helping users manage their dashboard effectively.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'python api clear dashboard example',\n",
       "  'summary_answer': 'This article explains how to clear all panels and tabs from a dashboard using the `clear_dashboard` method in Python.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'dashboard panel plot types',\n",
       "  'summary_answer': 'It describes different types of panels that can be added, such as text, counters, and plots including bar and line charts.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'add pie chart to dashboard',\n",
       "  'summary_answer': 'There are examples in the article on how to add pie chart panels to a dashboard using various metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'panel metric options',\n",
       "  'summary_answer': 'The article summarizes the parameters available for configuring `PanelMetric`, guiding how to specify what to display.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'multiple values in dashboard panel',\n",
       "  'summary_answer': 'It provides an example of how to display multiple values in a single panel, particularly with line charts.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'evidently metrics for dashboard',\n",
       "  'summary_answer': 'The article elaborates on how to reference and utilize metrics from Evidently in dashboard panels effectively.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'error handling when adding panels api',\n",
       "  'summary_answer': 'Users are guided on how to handle potential errors when trying to add panels or tabs that do not exist.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'api dashboard panel visualization settings',\n",
       "  'summary_answer': 'The article discusses visualization settings for dashboard panels, including how to configure the plot types and sizes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels.mdx'},\n",
       " {'question': 'create custom panels in dashboard',\n",
       "  'summary_answer': \"The article explains how to create custom panels in a dashboard by entering 'Edit' mode, clicking 'Add Panel', and following prompts to configure it with relevant metrics.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'adding tabs in dashboard',\n",
       "  'summary_answer': \"It details the steps to add tabs in a dashboard, including entering 'Edit' mode and using the 'add Tab' option to organize panels.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'using pre-built dashboard tabs',\n",
       "  'summary_answer': 'The article describes how to use pre-built tabs which come with preset panel combinations and rely on having the necessary metrics available.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'panel configuration in dashboard',\n",
       "  'summary_answer': 'It provides advanced tips for configuring panels, such as filtering metrics by tags and adjusting plot types, to effectively display data.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_add_panels_ui.mdx'},\n",
       " {'question': 'Dashboard features in Evidently',\n",
       "  'summary_answer': 'The article explains the main features of the Dashboard in Evidently, which allows users to track AI application performance through evaluations and live data quality monitoring.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/dashboard_overview.mdx'},\n",
       " {'question': 'How to add Panels to Dashboard using Python API',\n",
       "  'summary_answer': 'It details the process of adding Panels to the Dashboard using the Python API, including the necessary parameters and customization options for displaying metrics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/dashboard_overview.mdx'},\n",
       " {'question': 'create dataset in Evidently OSS',\n",
       "  'summary_answer': 'The article explains various methods to create datasets in Evidently OSS, including uploading CSV files through the UI or Python API, and generating synthetic data.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/datasets_overview.mdx'},\n",
       " {'question': 'what is synthetic data in Evidently',\n",
       "  'summary_answer': 'The article defines synthetic data as datasets generated directly in Evidently Cloud, with options to create them from examples or source documents, highlighting their importance in evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/datasets_overview.mdx'},\n",
       " {'question': 'upload dataset to Evidently',\n",
       "  'summary_answer': 'The article details how to prepare and upload a dataset to an Evidently project using the `add_dataset` method in Python or through the UI.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/datasets_workflow.mdx'},\n",
       " {'question': 'how to include dataset in Reports Evidently',\n",
       "  'summary_answer': 'It explains how to include datasets when uploading reports, using the `include_data` parameter to simultaneously upload the report and associated dataset.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/datasets_workflow.mdx'},\n",
       " {'question': 'run evals with Evidently API',\n",
       "  'summary_answer': 'The article provides a guide on using the Evidently API to run evaluations, detailing a simple code example for executing a text evaluation and logging it on the platform.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_api.mdx'},\n",
       " {'question': 'Evidently eval workflow steps',\n",
       "  'summary_answer': 'The document outlines the complete workflow for running evals, including steps like configuring a report, uploading results, exploring outcomes, and optionally setting up dashboards and alerts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_api.mdx'},\n",
       " {'question': 'view evaluation results on platform',\n",
       "  'summary_answer': 'The article explains how to access and explore evaluation results by navigating to the Reports section of your project and using the Explore view to analyze metrics and datasets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_explore.mdx'},\n",
       " {'question': 'no code data evaluation steps',\n",
       "  'summary_answer': 'The article outlines steps for evaluating data without coding, including dataset preparation, adding descriptors, and running evaluations using various methods.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'how to prepare dataset for no code evals',\n",
       "  'summary_answer': 'To prepare a dataset in a no-code interface, you can either upload a CSV file or use an existing dataset from the platform.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'add descriptors in no code evaluations',\n",
       "  'summary_answer': 'You can add descriptors to your dataset evaluation through the user interface, allowing you to choose evaluation methods and specify checks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'using LLM for no code evaluations',\n",
       "  'summary_answer': 'The article explains how to integrate an LLM provider API key for evaluations, allowing you to classify or score texts directly using LLM models.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_no_code.mdx'},\n",
       " {'question': 'evals workflow stages AI product',\n",
       "  'summary_answer': 'The article outlines different evaluation stages in AI product development, including ad hoc analysis, experiments, and monitoring, highlighting the importance of each step.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/evals_overview.mdx'},\n",
       " {'question': 'run evals locally Evidently API examples',\n",
       "  'summary_answer': 'The article explains how to execute evaluations via the Evidently API, offering Python-based workflows for experiments and CI/CD integrations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/evals_overview.mdx'},\n",
       " {'question': 'batch evaluation job example in Evidently',\n",
       "  'summary_answer': 'The article provides a simple code example demonstrating how to run a batch evaluation job using the Dataset and Report classes from the Evidently library, illustrating how to upload dataset stats to the workspace.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_local_batch.mdx'},\n",
       " {'question': 'Evidently batch monitoring workflow steps',\n",
       "  'summary_answer': 'The article outlines the complete workflow for batch monitoring, including steps for configuring metrics, running evaluations, uploading results, and setting up dashboards and alerts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/monitoring_local_batch.mdx'},\n",
       " {'question': 'AI quality monitoring overview',\n",
       "  'summary_answer': 'The article explains AI observability, detailing how quality monitoring facilitates evaluating inputs and outputs of AI applications in production.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'batch monitoring setup with Evidently',\n",
       "  'summary_answer': 'It describes how to set up batch monitoring jobs using Evidently, including building evaluation pipelines and running metric calculations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'tracing with scheduled evaluations',\n",
       "  'summary_answer': 'The article outlines how to implement tracing in LLM-powered applications by capturing relevant data and scheduling evaluations using the Evidently platform.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/monitoring_overview.mdx'},\n",
       " {'question': 'Evidently Platform features overview',\n",
       "  'summary_answer': 'The article summarizes key features of the Evidently Platform, including various evaluation methods, dataset management, synthetic data generation, and monitoring capabilities.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/overview.mdx'},\n",
       " {'question': 'Using Evidently Python library for evaluations',\n",
       "  'summary_answer': 'The article describes how to run evaluations locally with the Evidently Python library, highlighting no-code options and tracking capabilities for experiments.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/overview.mdx'},\n",
       " {'question': 'create project in Evidently',\n",
       "  'summary_answer': 'To create a project, you can either use Python code or the UI, with options for specifying an organization when using the cloud.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'how to connect to an existing project',\n",
       "  'summary_answer': 'You can connect to an existing project in Python using the `get_project` method along with the project ID.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'delete project using UI',\n",
       "  'summary_answer': \"To delete a project using the UI, hover over the project and click the 'delete' button, but note that this action removes all associated data.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'project parameters explained',\n",
       "  'summary_answer': 'The article lists the parameters associated with a project, including name, ID, description, and dashboard configuration, providing a clear understanding of their roles.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_manage.mdx'},\n",
       " {'question': 'how to organize projects in Evidently',\n",
       "  'summary_answer': 'The article explains how to structure Projects in Evidently based on various criteria like application, model, test scenario, phase, or use case to optimize data management and evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/projects_overview.mdx'},\n",
       " {'question': 'LLM tracing overview',\n",
       "  'summary_answer': 'The article defines LLM tracing as a method to instrument AI applications for detailed data collection on their operations, highlighting its importance for performance evaluation and analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_overview.mdx'},\n",
       " {'question': 'install tracely package example',\n",
       "  'summary_answer': 'The article provides a quick installation command for the `tracely` package using pip: `pip install tracely`.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'how to initialize tracing with tracely',\n",
       "  'summary_answer': 'To initialize tracing, use the `init_tracing` function with parameters such as `address`, `api_key`, and `project_id` as described in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'tracing function arguments in tracely',\n",
       "  'summary_answer': 'The article outlines the parameters for the `init_tracing()` function, including `address`, `api_key`, `project_id`, and their corresponding environment variables.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'use trace_event decorator in tracely',\n",
       "  'summary_answer': 'You can use the `@trace_event` decorator to collect traces for specific functions in your application, as explained with examples in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'environment variables for tracely',\n",
       "  'summary_answer': 'The article explains how to set parameters like `api_key` and `export_name` using environment variables which correspond to the function arguments.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'get export_id for tracing dataset',\n",
       "  'summary_answer': 'To retrieve the `export_id` of your tracing dataset, use the `get_info()` function provided in the `tracely` package as mentioned in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'creating nested events with tracely',\n",
       "  'summary_answer': 'You can trace multi-step workflows by using nested `@trace_event` decorators, where each function will automatically appear as a child span of the parent trace, as described in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'context manager for tracing in tracely',\n",
       "  'summary_answer': 'The article describes how to use a context manager to create trace events, allowing for more control over tracing specific blocks of code without decorators.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'set attributes in span using tracely',\n",
       "  'summary_answer': 'You can add attributes to the current span using the `get_current_span()` method to access the active span and set attributes, as shown in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'linking events across different systems tracely',\n",
       "  'summary_answer': 'The article explains how to connect events across systems into a single trace using the `tracely.bind_to_trace` method, allowing for complex tracing scenarios.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/platform/tracing_setup.mdx'},\n",
       " {'question': 'setup Evidently Cloud account steps',\n",
       "  'summary_answer': 'The article outlines how to create an Evidently Cloud account, generate an API token, and connect from Python, providing detailed instructions for beginners.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/cloud.mdx'},\n",
       " {'question': 'self-host Evidently UI service setup',\n",
       "  'summary_answer': 'The article explains the steps to set up the Evidently UI service for self-hosting, including creating workspaces and launching the service.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'how to create a workspace for Evidently',\n",
       "  'summary_answer': 'It details the process of creating a local or remote workspace to store evaluation results, necessary for utilizing the UI service effectively.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'Evidently UI local vs remote workspace',\n",
       "  'summary_answer': 'The article compares local and remote workspaces, explaining the differences in setup and data storage options for the Evidently UI service.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'launch Evidently UI service command',\n",
       "  'summary_answer': 'Instructions are provided on the terminal commands required to launch the Evidently UI service depending on workspace configurations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'Evidently UI delete workspace command',\n",
       "  'summary_answer': 'The article warns about the consequences of deleting a workspace and provides the necessary command to do so, emphasizing data loss.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'docs/setup/self-hosting.mdx'},\n",
       " {'question': 'automated testing LLM outputs GitHub Actions',\n",
       "  'summary_answer': 'The article explains how to set up automated testing of LLM outputs using Evidently in GitHub Actions, detailing dataset definitions, evaluation methods, and reporting results in CI.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/GitHub_actions.mdx'},\n",
       " {'question': 'LLM evaluation methods overview',\n",
       "  'summary_answer': 'The article outlines different evaluation methods for LLMs, including API setup and various metrics used for assessment.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_evals.mdx'},\n",
       " {'question': 'reference-free evaluation examples',\n",
       "  'summary_answer': 'The article discusses reference-free evaluation techniques such as text statistics and ML models, providing code examples for implementation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_evals.mdx'},\n",
       " {'question': 'evaluate text with LLM judge',\n",
       "  'summary_answer': 'The article explains how to use an LLM as a judge for evaluating text based on reference and custom criteria, providing coding examples for implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'setting up LLM evaluator',\n",
       "  'summary_answer': 'Instructions are included on how to set up and run an LLM evaluator, starting with prompt design and dataset creation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'create evaluation dataset for LLM',\n",
       "  'summary_answer': \"You can create a toy dataset to evaluate the LLM judge's performance by defining questions, target responses, and manual labels for accuracy assessment.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'openai API key for LLM',\n",
       "  'summary_answer': 'The article details how to pass your OpenAI API key as an environment variable to access the LLM evaluator.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'reference-based evaluator examples',\n",
       "  'summary_answer': 'Examples of creating a reference-based evaluator using the LLM judge are presented, which involves comparing new responses against predefined correct answers.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'how to score data with LLM judge',\n",
       "  'summary_answer': 'The tutorial describes how to score your data using the LLM judge by integrating evaluation descriptors into your dataset.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'custom criteria for LLM evaluation',\n",
       "  'summary_answer': 'You can define custom evaluation criteria for the LLM judge to assess outputs when no direct reference is available, enhancing flexibility.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'installing Evidently for LLM',\n",
       "  'summary_answer': 'Installation instructions for the Evidently package needed to run the LLM judge are provided, including the pip command.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'preview LLM judge results',\n",
       "  'summary_answer': 'The article explains how to preview results of the LLM judge using Pandas DataFrame in Python after running evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'LLM judge accuracy evaluation',\n",
       "  'summary_answer': 'The tutorial assesses the accuracy of the LLM judge by comparing its predictions with manual labels, providing insights into performance.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'verbosity evaluator setup',\n",
       "  'summary_answer': 'Instructions for setting up a verbosity evaluator with the LLM judge are included, focusing on checking response conciseness.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'report generation for LLM evaluations',\n",
       "  'summary_answer': 'You will learn to generate reports summarizing the evaluation results of the LLM judge and how to visualize them using Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'create a toy Q&A dataset',\n",
       "  'summary_answer': 'Steps to create a toy Q&A dataset for testing the LLM judge are detailed, including sample questions and responses.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'using LLMEval for judging',\n",
       "  'summary_answer': 'The article guides you on utilizing LLMEval to implement evaluations with the LLM judge, providing a practical template.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'integrating LLM judge into workflows',\n",
       "  'summary_answer': 'Tips are provided on integrating the LLM judge into evaluation workflows to enhance testing of prompts and outputs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'descriptors for LLM evaluation',\n",
       "  'summary_answer': 'You will learn about adding evaluation descriptors to datasets for enhanced analysis of LLM predictions in your model.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'evidently cloud platform integration',\n",
       "  'summary_answer': 'Information on how to connect your LLM evaluations with the Evidently Cloud platform for better tracking and reporting of results is presented.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'managing evaluation datasets',\n",
       "  'summary_answer': 'The tutorial teaches how to manage and explore your evaluation datasets systematically within the context of LLM judging.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'analyzing confusion matrix for LLM performance',\n",
       "  'summary_answer': 'The article includes details on analyzing the confusion matrix to evaluate the performance of the LLM judge further.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'is LLM judge reliable?',\n",
       "  'summary_answer': 'Insights into the reliability and accuracy of the LLM judge are shared, along with metrics to assess its performance.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'question': 'LLM evaluation approach',\n",
       "  'summary_answer': 'The article details an evaluation method using multiple LLMs to assess outputs and provides guidance on how to aggregate results or uncover disagreements.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'how to set up LLM judges',\n",
       "  'summary_answer': 'It explains how to configure evaluator LLMs by passing API keys and creating a project in the Evidently Cloud workspace.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'examples of email evaluation',\n",
       "  'summary_answer': 'The article provides toy datasets with user intents and model-generated emails used for judging the appropriateness of email content.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'installing Evidently',\n",
       "  'summary_answer': 'It outlines the installation process for Evidently and other necessary components needed for LLM evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'criteria for evaluating emails',\n",
       "  'summary_answer': 'The criteria for what constitutes an appropriate email are defined using the `BinaryClassificationPromptTemplate` in the code example.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'steps to create LLM evaluation report',\n",
       "  'summary_answer': 'The article provides detailed steps for running evaluations, exporting data, and generating summary reports for LLM judgments.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'understanding judge disagreements',\n",
       "  'summary_answer': 'The article describes how to implement custom descriptors to flag disagreements between LLM assessments in email evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'Evidently Cloud workspace setup',\n",
       "  'summary_answer': 'It discusses optional configuration of an Evidently Cloud workspace for storing and exploring evaluation results.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'aggregate LLM outputs',\n",
       "  'summary_answer': 'The article explains how to aggregate outputs from multiple LLMs to determine a consensus on email appropriateness.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'question': 'RAG system evaluation metrics',\n",
       "  'summary_answer': 'The article explains the metrics used to evaluate RAG systems, focusing on both retrieval and generation quality, and provides code examples for implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'how to install Evidently for RAG',\n",
       "  'summary_answer': 'Instructions for installing Evidently, including the necessary import statements and setting the OpenAI key, are provided in the tutorial.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'multiple context retrieval RAG evaluation',\n",
       "  'summary_answer': 'The article discusses how to evaluate the relevance of multiple contexts retrieved by a RAG system, with examples of how to implement this using Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'evaluate context quality in RAG',\n",
       "  'summary_answer': 'It shows how to assess overall context quality for single and multiple contexts using descriptors in Evidently, with code snippets provided for clarity.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'generate structured reports for RAG',\n",
       "  'summary_answer': 'The tutorial describes how to generate structured reports in Evidently to show RAG performance evaluation results, including code snippets for report creation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'what are ground truth datasets in RAG',\n",
       "  'summary_answer': 'The article explains the concept of ground truth datasets, how they are used in evaluating the RAG system, and methods to compare generated outputs against them.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'faithfulness evaluation in RAG',\n",
       "  'summary_answer': 'Instructions on evaluating faithfulness of generated responses in RAG systems are provided, along with code examples for this evaluation method.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'relevance score aggregation methods',\n",
       "  'summary_answer': 'The article discusses different aggregation methods for scoring relevance in RAG evaluations, comparing methods like hit rate and mean relevance with practical coding examples.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'using CorrectnessLLMEval in RAG',\n",
       "  'summary_answer': 'It details how to use CorrectnessLLMEval for assessing generated responses in RAG evaluations, providing necessary code snippets for implementation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'how to upload RAG evaluation results',\n",
       "  'summary_answer': 'The tutorial explains the procedure for uploading evaluation results to the Evidently Cloud platform, enhancing tracking and simplicity in sharing results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'Evidently Cloud project setup',\n",
       "  'summary_answer': 'Instructions on setting up a project in Evidently Cloud for RAG evaluations are provided, helping users manage their projects efficiently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'overview of RAG evaluation methods',\n",
       "  'summary_answer': 'The article provides an overview of various evaluation methods for RAG systems, highlighting their importance and practical implementation through code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'customizing LLM descriptor prompts',\n",
       "  'summary_answer': 'Instructions are provided on how to customize prompts for LLM descriptors in RAG evaluations, enhancing the specificity of evaluations in Evidently.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_rag_evals.mdx'},\n",
       " {'question': 'regression testing LLM outputs',\n",
       "  'summary_answer': 'The article explains the method of comparing new and old outputs from LLM to ensure consistency and correctness through regression testing.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'create toy dataset for LLM',\n",
       "  'summary_answer': 'Learn how to create a toy dataset with questions and reference answers as a starting point for regression testing LLM outputs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'Evidently Cloud setup for LLM',\n",
       "  'summary_answer': 'Instructions are provided for setting up and connecting to Evidently Cloud for running tests and tracking results of LLM outputs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'analyze LLM output changes',\n",
       "  'summary_answer': 'The article covers techniques to analyze changes in LLM outputs by running evaluations and comparing responses to previous outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'import required Python modules for LLM testing',\n",
       "  'summary_answer': 'To set up the regression testing environment, the article lists the necessary Python imports for using Evidently and LLM evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'define correctness judge for LLM',\n",
       "  'summary_answer': 'The article demonstrates how to implement a correctness judge using a binary classification template to evaluate LLM responses against reference answers.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'maximum length check for LLM responses',\n",
       "  'summary_answer': 'Details are provided on how to set up a maximum length check to ensure LLM responses do not exceed a defined character limit.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'view regression test reports',\n",
       "  'summary_answer': 'Learn how to generate and view reports on regression tests conducted on LLM outputs to assess performance and correctness over time.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'run tests with LLM evaluator',\n",
       "  'summary_answer': 'Guidance is given on running tests using an LLM evaluator to check for correctness and style consistency in new responses generated from prompts.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'monitoring dashboards for LLM testing',\n",
       "  'summary_answer': 'The article offers instructions on how to create monitoring dashboards to visualize LLM test results and track performance over time.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'install Evidently for LLM',\n",
       "  'summary_answer': 'The article includes the command to install the Evidently library to support regression testing for LLM outputs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'add descriptive statistics for LLM output',\n",
       "  'summary_answer': 'It discusses adding descriptive statistics to enhance the understanding of LLM output performance using the Evidently framework.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'initialize project in Evidently Cloud',\n",
       "  'summary_answer': 'Instructions for initializing a new project in Evidently Cloud are provided for tracking regression test results.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'use LLM judge for style checks',\n",
       "  'summary_answer': 'The article explains how to implement a style judge to evaluate the stylistic consistency of LLM responses.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'test LLM outputs with custom criteria',\n",
       "  'summary_answer': 'Learn about creating custom test criteria using the LLM-as-a-judge approach to evaluate the generated outputs more effectively.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'evaluate dataset against benchmarks',\n",
       "  'summary_answer': 'The article outlines methods to evaluate new LLM output datasets against established benchmarks for consistency and correctness.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'add tags to LLM test reports',\n",
       "  'summary_answer': 'Instructions are given on how to use tags in test reports to associate specific runs with particular parameters or versions.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'generate test results summary',\n",
       "  'summary_answer': 'The process of generating a summary of test results across multiple runs to assess improvements or regressions in LLM outputs is explained.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'set up CI/CD for LLM testing',\n",
       "  'summary_answer': 'The article provides insights on how to integrate LLM testing into CI/CD workflows to automate evaluation on every change.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'create automated dashboards for LLM test results',\n",
       "  'summary_answer': 'Learn how to create automated dashboards that update with new test results for continuous monitoring of LLM output validity.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'question': 'LLM evaluation quickstart',\n",
       "  'summary_answer': 'The article provides a quickstart guide that outlines the initial steps to evaluate LLM quality, suitable for new users.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'ML data drift evaluation steps',\n",
       "  'summary_answer': 'It includes a quickstart for testing tabular data quality and data drift for beginners to understand key concepts.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'How to evaluate text outputs LLM',\n",
       "  'summary_answer': 'There are end-to-end examples demonstrating how to evaluate the quality of text outputs from large language models.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'LLM judge tutorial',\n",
       "  'summary_answer': 'The article features a tutorial on creating and evaluating an LLM judge using human labels, ideal for those new to the topic.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Using multiple LLMs to evaluate outputs',\n",
       "  'summary_answer': 'A tutorial demonstrates how to employ multiple LLMs as judges to assess the same output, suitable for intermediate learners.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'LLM evaluation methods explained',\n",
       "  'summary_answer': 'It walks through various evaluation methods, offering insights into reference-based and reference-free evaluations of LLM outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'GitHub actions for LLM integration',\n",
       "  'summary_answer': 'The article details how to integrate Evidently evaluations within GitHub actions for automated testing in CI/CD workflows.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Adversarial testing for LLMs',\n",
       "  'summary_answer': 'A tutorial highlights how to conduct scenario-based adversarial testing on LLMs, focusing on brand risks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Optimizing LLM judge prompts',\n",
       "  'summary_answer': 'It explains how to optimize prompts for multi-class and binary classifiers using target labels and free-form feedback, providing practical code examples.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Overview of metrics in ML',\n",
       "  'summary_answer': 'The article features various metrics used in machine learning, including regression and classification metrics, laid out in an easy-to-follow format.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Framework for RAG evaluations',\n",
       "  'summary_answer': 'It gives a detailed breakdown of RAG evaluations, including methods to evaluate retrieval and generative quality.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'examples/introduction.mdx'},\n",
       " {'question': 'Evidently Cloud v2 updates and changes',\n",
       "  'summary_answer': 'The article details significant improvements in Evidently Cloud v2, including a redesigned dashboard and performance enhancements, along with breaking changes that require users to adapt their SDK version.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/cloud_v2.mdx'},\n",
       " {'question': 'migrating to Evidently 0.6 changes',\n",
       "  'summary_answer': 'The guide details key changes introduced in Evidently 0.6, including a new API and the introduction of the Report object.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'new API in Evidently 0.6',\n",
       "  'summary_answer': 'Version 0.6 features a new core API where you need to import components from evidently.future, significantly changing how reports and datasets are handled.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'old API compatibility Evidently 0.6 to 0.6.7',\n",
       "  'summary_answer': 'During the transition from version 0.6 to 0.6.7, both the new and legacy APIs coexist, allowing for flexibility in migrating to the updated system.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'how to create Dataset in Evidently',\n",
       "  'summary_answer': 'You now need to explicitly create a Dataset object in the new Evidently API, replacing the previous method of directly passing a DataFrame.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'using descriptors in new Evidently version',\n",
       "  'summary_answer': 'The article explains how descriptors have been updated, allowing you to perform evaluations and checks in a more modular approach with improved APIs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'Reports API changes Evidently 0.6',\n",
       "  'summary_answer': 'The Reports API has undergone significant changes including the unification of Reports and Tests, simplifying the way metrics and evaluations are managed together.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'Evidently Reports and Tests unified',\n",
       "  'summary_answer': 'The guide explains the new Test Suite mode, which integrates test results into the same Report, improving usability and eliminating duplication.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'question': 'Evidently library features overview',\n",
       "  'summary_answer': 'The Evidently library offers various data evaluations, reports, and test suites, being suitable for data scientists and AI engineers in Python environments.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Cloud vs open-source Evidently features',\n",
       "  'summary_answer': 'The article outlines the core and premium features available in both the open-source and cloud versions of the Evidently platform, highlighting differences in functionality and support.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Tracely library open-source details',\n",
       "  'summary_answer': 'Tracely is an open-source Python library that captures real-time data from AI applications and integrates with OpenTelemetry.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Evidently Enterprise vs Cloud features',\n",
       "  'summary_answer': 'The article compares Evidently Cloud and the Enterprise versions, detailing their deployment options and additional features such as role-based access control and premium capabilities.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Deploy Evidently OSS',\n",
       "  'summary_answer': 'To deploy the Evidently OSS version, users need to manage their own hosting environment, including maintenance tasks like backups and updates.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'Benefits of Evidently Cloud',\n",
       "  'summary_answer': 'The Evidently Cloud platform is fully managed, offering automatic updates and scalability, making it an efficient choice for teams focusing on AI development.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/oss_vs_cloud.mdx'},\n",
       " {'question': 'telemetry data collection evidently',\n",
       "  'summary_answer': 'The article outlines the types of telemetry data collected by Evidently, including environment and service usage data, while ensuring anonymity.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'how to disable telemetry in evidently',\n",
       "  'summary_answer': 'The article provides steps to disable telemetry by setting the environment variable `DO_NOT_TRACK` to any value.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'is evidently telemetry anonymous',\n",
       "  'summary_answer': 'Evidently collects only anonymous usage data, ensuring personal information is not collected.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'evidently telemetry data examples',\n",
       "  'summary_answer': 'The article includes specific examples of telemetry data collected during actions like startup, indexing, and listing projects.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'what data does evidently collect',\n",
       "  'summary_answer': 'The article describes the types of data collected by Evidently, including user environment details and service usage actions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'how to enable telemetry evidently',\n",
       "  'summary_answer': 'To enable telemetry, simply unset the environment variable `DO_NOT_TRACK`. The default setting is telemetry enabled.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'evidently user id handling telemetry',\n",
       "  'summary_answer': 'The user ID collected in telemetry is anonymized to ensure privacy while still allowing for user action tracking.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'importance of telemetry in evidently',\n",
       "  'summary_answer': 'Telemetry helps the developers understand user engagement, which features are popular, and how to prioritize new developments in Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'evidently source ip collection',\n",
       "  'summary_answer': 'Evidently obscures the exact source IP address by using `jitsu`, an open-source tool for event collection, ensuring user privacy.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'evidently event log structure',\n",
       "  'summary_answer': 'The article presents a structured example of telemetry event logs, showcasing the data format and key-value pairs collected during usage.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/telemetry.mdx'},\n",
       " {'question': 'benefits of using Evidently AI',\n",
       "  'summary_answer': 'Evidently AI provides a flexible, model-agnostic platform with built-in evaluations for reliable AI product development, allowing users to build without limitations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'Evidently features and evaluations',\n",
       "  'summary_answer': 'Evidently offers over 100 built-in evaluations covering various ML and LLM use cases, making it easy for developers to assess model performance without extensive setup.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'open-source capabilities of Evidently',\n",
       "  'summary_answer': 'As an open-source library with extensive community support, Evidently allows developers to inspect metrics implementations and utilize an intuitive API to enhance their AI workflows.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'integration options with Evidently platform',\n",
       "  'summary_answer': 'Evidently integrates seamlessly with existing tools, facilitating easy export of metrics and reports, which enhances collaborative workflows among team members.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'faq/why_evidently.mdx'},\n",
       " {'question': 'Evidently Python library examples',\n",
       "  'summary_answer': 'The article details how Evidently as a Python library helps in evaluating AI systems with over 100 evaluation metrics and provides examples in the Cookbook section for practical implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'introduction.mdx'},\n",
       " {'question': 'row-level text evaluations',\n",
       "  'summary_answer': 'The article provides a comprehensive reference for performing row-level text evaluations and using different descriptors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'code examples for LLM evaluations',\n",
       "  'summary_answer': 'The article includes a section for code examples focused on LLM evaluations, guiding users through the implementation of various descriptors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'ExactMatch descriptor usage',\n",
       "  'summary_answer': 'The ExactMatch descriptor checks for matching contents between two columns and returns True or False, with syntax examples provided in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to validate JSON structure',\n",
       "  'summary_answer': 'To validate JSON structure, you can use the IsValidJSON descriptor, which checks if the content is a valid JSON and returns True or False based on validation results.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'measuring text length in Python',\n",
       "  'summary_answer': 'You can measure the length of a text using the TextLength descriptor, which returns the number of symbols present in the text.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'parameters for Contains descriptor',\n",
       "  'summary_answer': 'The Contains descriptor requires a list of items and checks if the text contains any or all of those items, with optional parameters for case sensitivity.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'using CustomDescriptor in evaluations',\n",
       "  'summary_answer': 'The CustomDescriptor allows users to implement their own checks for specific columns as a Python function, providing great flexibility in text evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'returns from DoesNotContain descriptor',\n",
       "  'summary_answer': 'The DoesNotContain descriptor returns True or False based on whether specified items are absent in the text.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'parameter options for JSONSchemaMatch',\n",
       "  'summary_answer': 'JSONSchemaMatch includes parameters like expected_schema and options for exact matching and type validation to ensure structured data integrity.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'context relevance with LLM',\n",
       "  'summary_answer': 'The article explains how to use ContextRelevance to check if context chunks are relevant to a given question, employing various methods for calculation.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'using ContextQualityLLMEval',\n",
       "  'summary_answer': 'ContextQualityLLMEval evaluates if context provides enough information to answer a question, returning a label or score based on the evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'syntax validation descriptors',\n",
       "  'summary_answer': 'Descriptors like IsValidPython and IsValidSQL are provided for validating the syntax of Python code and SQL queries, respectively.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to check for biased texts',\n",
       "  'summary_answer': 'The BiasLLMEval descriptor detects biased texts and returns a score or label indicating whether bias is present.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'measuring percentage of OOV words',\n",
       "  'summary_answer': 'The OOVWordsPercentage descriptor calculates how many out-of-vocabulary words are present in a text, returning a score from 0 to 100.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'function of ItemMatch descriptor',\n",
       "  'summary_answer': 'ItemMatch checks if any or all specified items are present in the corresponding row and returns True or False accordingly.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'Detecting negative tones in text',\n",
       "  'summary_answer': 'The NegativityLLMEval checks for negative tones within texts and evaluates them against user-defined criteria, offering scores or labels.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'code to check URL presence',\n",
       "  'summary_answer': 'Use the ContainsLink descriptor to verify if a column contains valid URLs, returning True or False for each row.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'parameters for CompletenessLLMEval',\n",
       "  'summary_answer': 'CompletenessLLMEval requires a context parameter and evaluates if a response fully utilizes the provided context to return a completeness score.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'text statistics evaluation',\n",
       "  'summary_answer': 'The document outlines various descriptors for descriptive statistics about text, such as word count and sentence count, to summarize text attributes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'LLM judge parameters',\n",
       "  'summary_answer': 'The article discusses how to specify LLM judge templates and their parameters to tailor evaluations effectively, ensuring flexibility in scoring criteria.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to use BeginsWith descriptor',\n",
       "  'summary_answer': 'BeginsWith checks if text starts with a specified prefix, with options for case sensitivity as optional parameters.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'how to implement a CustomColumnsDescriptor',\n",
       "  'summary_answer': 'CustomColumnsDescriptor allows for applying custom checks as a Python function, specifically designed to handle any column in your dataset.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'determine text sentiment',\n",
       "  'summary_answer': 'The Sentiment descriptor analyzes sentiment in text using a word-based model from NLTK, providing a score ranging from negative to positive.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'checking for PII in text',\n",
       "  'summary_answer': 'The PIILLMEval descriptor specifically detects any personally identifiable information within the text and returns appropriate labels.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_descriptors.mdx'},\n",
       " {'question': 'evidently metrics reference',\n",
       "  'summary_answer': 'The article serves as a detailed reference for various dataset-level evaluation metrics in machine learning, outlining their functions and usage.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to use TextEvals in evidently',\n",
       "  'summary_answer': 'TextEvals() is a large preset that summarizes text evaluation results by displaying ValueStats for all descriptors in a dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'parameters for ValueStats in evidently',\n",
       "  'summary_answer': \"ValueStats() requires a 'column' parameter and computes various descriptive statistics for that column, including unique and missing value counts.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'data quality metrics overview',\n",
       "  'summary_answer': 'The article outlines various data quality metrics that check for missing values, out-of-range values, and unique value counts at the column level.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'metric cookbook example',\n",
       "  'summary_answer': 'Users can reference the Metric cookbook for concrete code examples illustrating how to implement different metrics in their evaluations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'understanding test defaults in metrics',\n",
       "  'summary_answer': 'Test defaults in metrics determine conditions that apply during evaluation, which may vary based on whether a reference dataset is provided.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to calculate precision in evidently',\n",
       "  'summary_answer': 'Precision() calculates the proportion of true positive results among all positive identifications in a dataset, and requires specific visualizations to be set.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'evidently ColumnCount metric',\n",
       "  'summary_answer': 'ColumnCount() counts the total number of columns in a dataset, with an optional parameter for test conditions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'meaning of dummy model metrics',\n",
       "  'summary_answer': 'Dummy model metrics provide a baseline for model evaluation, comparing metrics such as precision and recall against a random or constant model.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to check for missing values in evidently',\n",
       "  'summary_answer': 'MissingValueCount() counts and reports the number and share of missing values for specified columns in a dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'data drift detection methods',\n",
       "  'summary_answer': 'The article describes various metrics used to detect data distribution drift for both text and tabular data, including details on calculation methods.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'mean error calculation in regression metrics',\n",
       "  'summary_answer': 'MeanError() computes the mean error and requires at least one visualization parameter to be set for effective output.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'usage of RecallTopK metric',\n",
       "  'summary_answer': 'RecallTopK() calculates the recall for the top K retrieved items to evaluate recommendation systems, while requiring specific parameters.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'importance of dataset stats',\n",
       "  'summary_answer': 'Dataset stats provide essential insights into column types, row counts, and data quality, essential for exploratory data analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'parameters for StandardDeviation in ValueStats',\n",
       "  'summary_answer': 'StandardDeviation is one of the metrics computed by ValueStats() and provides insights into the variance within a specified numerical column.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'exploratory data analysis metrics',\n",
       "  'summary_answer': 'The article outlines metrics applicable for exploratory data analysis, including those for checking data quality and distribution characteristics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to calculate percentage error with MAPE',\n",
       "  'summary_answer': 'MAPE() calculates the Mean Absolute Percentage Error, focusing on forecasting accuracy against median values in regression tasks.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'what is accuracy metric in classification',\n",
       "  'summary_answer': 'Accuracy() evaluates the overall correctness of predictions by calculating the portion of true results from total predictions made.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'comparing model performance with dummy models',\n",
       "  'summary_answer': 'Dummy models serve as a baseline for comparison, ensuring that trained models outperform these simple metrics as a part of evaluation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'evidently regression metrics overview',\n",
       "  'summary_answer': 'The article provides an overview of various regression metrics like RMSE, MAE, and R² that are crucial for assessing model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to check outliers with OutRangeValueCount',\n",
       "  'summary_answer': 'OutRangeValueCount() counts and reports the number and share of values that fall outside specified limits, useful in data validation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how tests work in evidently',\n",
       "  'summary_answer': 'Tests in evidently allow users to implement conditional checks during metric evaluations using operators like eq, gt, etc., for flexibility.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'importance of dummy model evaluation metrics',\n",
       "  'summary_answer': 'These metrics help track whether a new model performs better than a naive approach, serving a critical role in performance validation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'settings for DriftedColumnsCount',\n",
       "  'summary_answer': 'DriftedColumnsCount() determines the number of columns that show significant drift compared to a reference, indicating data stability.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'visualizing classification metrics results',\n",
       "  'summary_answer': 'Classification metrics can be visualized through confusion matrices and PR curves, enhancing interpretability and analysis.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'what is a category count metric',\n",
       "  'summary_answer': 'CategoryCount() counts occurrences of specified categories within a column, useful for understanding categorical distributions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to use QuantileValue in metrics',\n",
       "  'summary_answer': 'QuantileValue() computes the quantile of a specified numerical column, applying a default of 0.5 if no specific quantile is provided.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'dataset data quality metrics overview',\n",
       "  'summary_answer': 'Dataset-level data quality metrics assess overall data integrity, including constant columns and duplicate row counts.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'calculate F1 score with F1Score',\n",
       "  'summary_answer': \"F1Score() calculates the harmonic mean of precision and recall, providing a single metric for evaluating a classification task's accuracy.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to set parameters for InRangeValueCount',\n",
       "  'summary_answer': 'InRangeValueCount() counts the number of values falling between specified thresholds, helping with range validations in datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'evidently metrics for recommender systems',\n",
       "  'summary_answer': 'Metrics such as PrecisionTopK and RecallTopK are specified for evaluating the effectiveness of recommendation algorithms in finding relevant items.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'summarize regression metrics explained',\n",
       "  'summary_answer': 'The article details metrics like R² and Mean Absolute Error, outlining their calculation methods and significance in evaluating regression models.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'evidently Dummy model quality metrics explained',\n",
       "  'summary_answer': 'Dummy model metrics provide a baseline for model quality by using simple, heuristic calculations for comparison.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to assess data quality with ColumnsWithMissingValuesCount',\n",
       "  'summary_answer': 'ColumnsWithMissingValuesCount() quantifies the number of columns containing missing data, crucial for data cleaning processes.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'how to use Ranking metrics in evidently',\n",
       "  'summary_answer': 'Ranking metrics evaluate the effectiveness of recommendations and retrieval systems, focusing on metrics like NDCG and MAP.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/all_metrics.mdx'},\n",
       " {'question': 'overview of presets for evaluation',\n",
       "  'summary_answer': 'The article outlines various pre-built evaluation templates (Presets) that simplify the evaluation process for datasets without requiring setup.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/all_presets.mdx'},\n",
       " {'question': 'data drift detection methods',\n",
       "  'summary_answer': 'The article outlines various data drift detection methods and parameters to customize detection based on the column type and data characteristics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'customizing data drift parameters',\n",
       "  'summary_answer': 'You can customize data drift detection by passing specific parameters to metrics or presets, such as drift methods, thresholds, and column shares.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'how to set drift thresholds',\n",
       "  'summary_answer': 'It is possible to set thresholds for each drift detection method to enhance the specificity of drift detection in your dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'method for categorical data drift',\n",
       "  'summary_answer': 'The article explains how to choose drift detection methods for categorical columns, such as using the Chi-Square test for categorical data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'how to implement custom drift method',\n",
       "  'summary_answer': 'You can implement a custom drift detection method by defining a StatTest function and registering it with the required parameters.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'default data drift algorithm',\n",
       "  'summary_answer': 'The default data drift algorithm automatically selects the detection method based on the column type and volume of data.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'drift share parameter example',\n",
       "  'summary_answer': 'To set the drift share parameter, use the DataDriftPreset method with a specified drift share, e.g., 0.7 for detecting dataset drift.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'PSI method for drift detection',\n",
       "  'summary_answer': 'You can apply the Population Stability Index (PSI) method for detecting drift across all columns in the dataset by specifying it in the report.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'detecting drift in numerical columns',\n",
       "  'summary_answer': 'For numerical columns, several statistical tests like K-S and Wasserstein distance are available for effectively detecting data drift.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'text data drift detection methods',\n",
       "  'summary_answer': 'The article describes specific methods for detecting drift in raw text data using statistical hypothesis testing techniques.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'automated drift detection adjustments',\n",
       "  'summary_answer': 'You can override automated data drift detection settings by specifying custom algorithms or thresholds tailored to your dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'analyzing dataset drift',\n",
       "  'summary_answer': 'To analyze dataset drift, you need to assess the share of drifting columns in your metrics, which can be set as a parameter in the report.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'supported statistical tests for drift detection',\n",
       "  'summary_answer': 'The article lists various supported statistical tests for drift detection, categorizing them based on numerical or categorical data types.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'role of thresholds in drift detection',\n",
       "  'summary_answer': 'Thresholds in drift detection can vary in interpretation; some methods treat higher values as better, while others may mean worse results, depending on the context.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'how to use drift detection in python',\n",
       "  'summary_answer': 'Example code is provided to demonstrate how to use various drift detection methods in Python using the Evidently library.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'using ValueDrift metric',\n",
       "  'summary_answer': 'The ValueDrift metric allows for specifying drift detection methods and thresholds for particular columns individually in the dataset.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'drift detection for text descriptors',\n",
       "  'summary_answer': 'You can check for distribution drift in text descriptors by first computing the relevant text descriptors and then applying standard drift detection methods.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_data_drift.mdx'},\n",
       " {'question': 'custom text evaluator example python',\n",
       "  'summary_answer': 'The article provides Python code examples for creating custom text evaluators, such as a custom descriptor for checking if a column is empty or performing exact matches.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'how to use CustomColumnDescriptor Evidently',\n",
       "  'summary_answer': 'Instructions are included on how to implement and use CustomColumnDescriptor in your dataset evaluations, along with example code demonstrating its use.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'advanced custom descriptor features Evidently',\n",
       "  'summary_answer': 'The article delves into the advanced capabilities of CustomDescriptor, including multi-column checks and returning multiple scores from a dataset.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_descriptor.mdx'},\n",
       " {'question': 'using HuggingFace for text evaluation',\n",
       "  'summary_answer': 'The article details the process of employing HuggingFace models to evaluate and classify text data according to various criteria, such as emotions.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'HuggingFace descriptors for emotion classification',\n",
       "  'summary_answer': 'It discusses using specific descriptors from HuggingFace to classify texts based on emotions, providing code examples for implementation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'install HuggingFace model dependencies',\n",
       "  'summary_answer': 'The article assumes familiarity with Python and the necessary imports to work with HuggingFace models in your projects.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'example of using HuggingFace for text scoring',\n",
       "  'summary_answer': 'An example is provided that demonstrates how to score and classify text using the HuggingFace descriptors, including sample code.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'HuggingFace add_descriptors usage',\n",
       "  'summary_answer': 'You will learn how to add built-in evaluators using the `add_descriptors` method to evaluate text data with HuggingFace.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'custom evaluations HuggingFace',\n",
       "  'summary_answer': 'The article explains how to create custom checks by defining your own Python functions for evaluating texts with HuggingFace models.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'overview of HuggingFace models',\n",
       "  'summary_answer': 'It outlines different types of HuggingFace models available for text classification, including emotion and zero-shot classification models.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'HuggingFace parameters for emotion model',\n",
       "  'summary_answer': 'The article provides detailed parameters needed to specify when using the HuggingFace model for emotion classification, such as the label to evaluate.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'sample data for evaluating HuggingFace',\n",
       "  'summary_answer': 'A toy dataset example is provided in the article to run your evaluations with HuggingFace models, showing how to create a Dataset object.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'integrating HuggingFace with evidently library',\n",
       "  'summary_answer': 'Learn how to integrate HuggingFace models into your text evaluation processes using the Evidently library as described in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_hf_descriptor.mdx'},\n",
       " {'question': 'how to use LLM evaluators',\n",
       "  'summary_answer': 'The article explains how to utilize built-in LLM evaluators or configure custom criteria by using templates in Python.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'set up OpenAI API key for LLM',\n",
       "  'summary_answer': 'To use the LLM-based descriptors, you need to set your OpenAI API key as an environment variable as shown in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'write a binary classification prompt template',\n",
       "  'summary_answer': 'The article provides an example of creating a binary classification prompt template for evaluating text characteristics.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'evaluate text for toxicity automatically',\n",
       "  'summary_answer': 'You can run built-in ToxicityLLMEval on the response column to check for toxic content as demonstrated in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'implement a custom LLM judge',\n",
       "  'summary_answer': 'The article outlines steps to create a custom LLM evaluator by choosing templates and setting evaluation criteria.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'configure multi-class classification using LLM',\n",
       "  'summary_answer': 'You can use the multi-class classification prompt template to evaluate responses against multiple categories in your LLM integration.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'pass additional columns to LLM evaluation',\n",
       "  'summary_answer': 'The article explains how to include additional columns in LLM evaluations using placeholders to map them in the criteria.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'run single column evaluation with LLM',\n",
       "  'summary_answer': \"To evaluate a single column like 'response' for toxicity, the article provides code examples to add descriptors effectively.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'how to parameterize LLM evaluators',\n",
       "  'summary_answer': 'The article details how you can switch output formats and include reasoning or scores when using LLM evaluators.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'use ToxicityLLMEval in code',\n",
       "  'summary_answer': 'You can implement ToxicityLLMEval within Python code to assess the toxicity of provided text data, as shown in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'Evidently library for text evaluation',\n",
       "  'summary_answer': 'The article discusses utilizing the Evidently library for setting up prompt-based evaluators with LLMs.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'how to change LLM model',\n",
       "  'summary_answer': 'You can change the model used in evaluations by specifying the model name during the descriptor setup in your code.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'create a custom prompt for evaluation',\n",
       "  'summary_answer': 'The article provides guidance on crafting custom prompts based on templates to tailor LLM evaluations to your needs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'using models from different providers in LLM',\n",
       "  'summary_answer': 'You can select models from various providers by passing the correct provider name and model name parameters as explained in the article.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'steps to evaluate context quality in text',\n",
       "  'summary_answer': 'You can evaluate context quality by running ContextQualityLLMEval with the appropriate columns, as demonstrated with example code.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'understanding LLM-based descriptors in Evidently',\n",
       "  'summary_answer': 'The article explains the purpose and functionality of LLM-based descriptors within the Evidently library for evaluating text data.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'set context for custom LLM evaluations',\n",
       "  'summary_answer': 'To set context for LLM evaluations, update the prompt using pre-messages or specific criteria, as described in the article.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'Python imports for LLM evaluation',\n",
       "  'summary_answer': 'The article lists the necessary imports for setting up LLM evaluations in Python, including specific templates and descriptors.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'how to pass options to LLM providers',\n",
       "  'summary_answer': 'The article explains how to use options to directly pass API keys and other parameters to LLM providers during evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'get output from LLM evaluations',\n",
       "  'summary_answer': 'After running evaluations, you can view the results using the `as_dataframe()` method as shown in the code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'LLM evaluation for text responses',\n",
       "  'summary_answer': 'The article shows how to set up evaluations for text responses using binary and multi-class classifiers with LLMs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'customizing LLM evaluation criteria',\n",
       "  'summary_answer': 'You can customize the criteria for LLM evaluations by defining specific grading logic and categories based on your requirements.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'check built-in LLM evaluators in Evidently',\n",
       "  'summary_answer': 'The article describes how to access and utilize built-in LLM evaluators available in the Evidently library.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_llm_judge.mdx'},\n",
       " {'question': 'create custom metrics in data evaluation',\n",
       "  'summary_answer': 'The article explains how to create custom metrics for column or dataset evaluations, including the necessary implementation steps and options for customization.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'implement custom evaluation metrics',\n",
       "  'summary_answer': 'It details the process of implementing custom calculation methods and optional test conditions for evaluations, along with visualization options.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'using Plotly for custom metric visualizations',\n",
       "  'summary_answer': 'The article outlines how to integrate Plotly for visualizing custom metrics, showing examples of setting up visual outputs.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'default tests for custom metrics',\n",
       "  'summary_answer': 'It explains how to define default tests that apply to custom metrics when not overridden by custom conditions, ensuring reliable evaluations.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/customize_metric.mdx'},\n",
       " {'question': 'classification metrics overview',\n",
       "  'summary_answer': 'The article provides a comprehensive overview of key classification metrics such as accuracy, precision, and recall, crucial for model performance analysis.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'evidently classification model metrics examples',\n",
       "  'summary_answer': 'Evidently offers practical examples of how to compute and visualize classification metrics like precision and recall, helping users understand model performance effectively.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'importance of confusion matrix in classification',\n",
       "  'summary_answer': 'The article explains how a confusion matrix helps visualize classification errors and their types, which is vital for improving model outcomes.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'how to interpret ROC curve',\n",
       "  'summary_answer': 'The article describes how to interpret ROC curves and their significance in evaluating the trade-offs between true positive and false positive rates.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'precision-recall table for model evaluation',\n",
       "  'summary_answer': 'The precision-recall table presented in the article shows outcomes for different classification thresholds, allowing for detailed model evaluation based on prediction coverage.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'class separation quality visualization',\n",
       "  'summary_answer': 'This article discusses how the class separation quality visualization showcases correct and incorrect predictions to help in model threshold optimization.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'interactive visualizations for model performance',\n",
       "  'summary_answer': 'Evidently generates interactive visualizations for model performance analysis, aiding in spotting mistakes and developing improvement strategies.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'calculating F1-score details',\n",
       "  'summary_answer': 'The article touches on how to calculate the F1-score, highlighting its importance as a balance between precision and recall in model assessment.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_classification.mdx'},\n",
       " {'question': 'data quality summary widget usage',\n",
       "  'summary_answer': 'The article explains how the summary widget presents an overview of datasets, highlighting missing features and constant values.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'feature widget statistical summaries',\n",
       "  'summary_answer': 'It discusses how the features widget generates visualizations and statistical summaries for different types of features within a dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'visualizations for categorical vs numerical features',\n",
       "  'summary_answer': 'The article provides examples of visualizations created for both categorical and numerical features, showing how they differ in representation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'understanding feature behavior over time',\n",
       "  'summary_answer': 'It details how users can analyze feature behavior over time using additional visualizations available through the features widget.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'feature interaction with target variable',\n",
       "  'summary_answer': 'The article describes how features are visualized in relation to the target variable to understand their influence on outcomes.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'correlation insights between features',\n",
       "  'summary_answer': 'It outlines how the correlation widget summarizes pairwise correlations and identifies highly correlated variables in the dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'heatmaps for correlation analysis',\n",
       "  'summary_answer': 'The article mentions that the correlation widget includes heatmaps to visualize relationships between features, although some functionalities have been removed in newer versions.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_data_stats.mdx'},\n",
       " {'question': 'data drift detection algorithm',\n",
       "  'summary_answer': 'The article explains the default algorithm used by Evidently for detecting data drift, focusing on the distribution changes in individual columns of datasets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'how to detect data drift in datasets',\n",
       "  'summary_answer': 'Evidently detects data drift by comparing distributions of reference and current datasets using various statistical tests and methods as outlined in the article.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'default methods for data drift',\n",
       "  'summary_answer': 'The article lists default methods for data drift detection based on column types and dataset sizes, including tests like Kolmogorov-Smirnov and chi-squared.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'requirements for datasets in drift detection',\n",
       "  'summary_answer': 'To evaluate data drift, the article specifies that two datasets (current and reference) must be non-empty and meet other criteria to avoid errors in drift calculations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'visualizing data drift distributions',\n",
       "  'summary_answer': 'The article describes how to visualize the distribution of columns in data drift analysis using Evidently’s tools, including mean plots and standard deviations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'custom thresholds for data drift',\n",
       "  'summary_answer': 'It explains how users can set custom thresholds for data drift detection methods in Evidently, adapting the default algorithms as needed.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'impact of empty values on drift detection',\n",
       "  'summary_answer': 'The article notes that columns with empty values can lead to errors in drift detection and recommends running separate tests on null shares in the dataset.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'text data drift classification',\n",
       "  'summary_answer': 'The article explains how Evidently uses a domain classifier approach to detect drift in text data, comparing ROC AUC scores to detect significant differences.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_drift.mdx'},\n",
       " {'question': 'Recall at K metric definition',\n",
       "  'summary_answer': \"Recall at K measures how many relevant items are present in the top K results of a recommendation system, indicating the system's ability to retrieve relevant items.\",\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Calculating precision at K',\n",
       "  'summary_answer': 'Precision at K calculates the proportion of relevant results in the top K recommended items, measuring the quality of suggestions made by the system.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'F Beta score calculation',\n",
       "  'summary_answer': \"The F Beta score combines precision and recall into a single metric, allowing for a balanced evaluation of a ranking system's performance depending on the value of Beta used.\",\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Mean Average Precision formula',\n",
       "  'summary_answer': 'Mean Average Precision (MAP) assesses how well a system ranks relevant items, focusing on their positions in the top K results, and averages the precision calculated for each relevant item.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Interpreting Mean Average Recall',\n",
       "  'summary_answer': 'Mean Average Recall (MAR) provides insight into how effectively a recommendation system retrieves relevant items across different users, averaging the recall calculations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'How to compute Normalized Discounted Cumulative Gain',\n",
       "  'summary_answer': 'NDCG compares the ranking quality of recommended items against an ideal order, helping assess the effectiveness of a ranking algorithm in placing relevant items higher.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Understanding Hit Rate',\n",
       "  'summary_answer': 'Hit Rate calculates the percentage of users for whom at least one relevant item appears in the top K suggestions, showcasing overall system effectiveness.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Mean Reciprocal Rank explained',\n",
       "  'summary_answer': 'Mean Reciprocal Rank (MRR) focuses on the position of the first relevant item in the recommendation list, measuring how closely relevant items appear at the top.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Score Distribution entropy calculation',\n",
       "  'summary_answer': 'Score Distribution measures the entropy of predicted scores for recommendations, helping visualize how scores are distributed among items in the top K.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'Differences between precision and recall',\n",
       "  'summary_answer': 'Precision and recall are critical metrics for evaluating recommendation systems, where precision measures the relevance of the suggested items and recall assesses the retrieval of all relevant items.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_recsys.mdx'},\n",
       " {'question': 'mean error and absolute error differences',\n",
       "  'summary_answer': 'The article explains Mean Error (ME), Mean Absolute Error (MAE), and their significance in evaluating model performance.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'evidently regression metrics examples',\n",
       "  'summary_answer': 'It provides examples of several regression metrics like MAPE, along with how to calculate and interpret them using Evidently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'how to visualize predicted vs actual in regression',\n",
       "  'summary_answer': 'The article showcases methods to create scatter plots for visualizing predicted versus actual values to assess model accuracy.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'error distribution of regression model',\n",
       "  'summary_answer': 'It describes how to analyze the distribution of errors in regression models to understand performance stability.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'interactivity in regression analysis with evidently',\n",
       "  'summary_answer': 'Evidently offers interactive visualizations that help in analyzing model predictions and identifying areas for improvement.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'absolute percentage error time series',\n",
       "  'summary_answer': 'The article discusses how to visualize absolute percentage error over time to track prediction accuracy across periods.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'explore error bias among different feature values',\n",
       "  'summary_answer': 'It emphasizes checking error bias by investigating feature distributions across groups with extreme errors.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'link between feature values and regression errors',\n",
       "  'summary_answer': 'The article explores the relationship between feature values and model errors, suggesting how to analyze them for better predictions.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'calculating range% between overestimations and underestimations',\n",
       "  'summary_answer': 'It explains the formula and process to calculate the range percentage when evaluating the differences between over and underestimations in predictions.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/explainer_regression.mdx'},\n",
       " {'question': 'classification preset report example',\n",
       "  'summary_answer': 'The article includes Python examples for setting up a Report using the ClassificationPreset to evaluate performance on classification tasks.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'metrics for classification quality',\n",
       "  'summary_answer': 'It discusses various metrics like Accuracy, Precision, Recall, and F1-score used to evaluate classification model performance.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'customizing classification report tests',\n",
       "  'summary_answer': 'The article explains how to customize test conditions and metrics in a classification report to better evaluate model performance against specific criteria.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_classification.mdx'},\n",
       " {'question': 'data drift preset overview',\n",
       "  'summary_answer': 'The article gives a comprehensive overview of the Data Drift Preset, detailing its function in evaluating shifts in data distribution between datasets.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'how to detect data drift',\n",
       "  'summary_answer': 'It explains how to implement the DataDriftPreset in a report to run data drift evaluations between current and reference datasets with simple code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'customize data drift report',\n",
       "  'summary_answer': 'The article describes various customization options for the Data Drift report, including selecting specific columns, adjusting drift detection parameters, and adding other metrics like data quality checks.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'data drift test suite example',\n",
       "  'summary_answer': 'It provides an example of using a test suite to add explicit pass/fail tests for each column when evaluating data drift, enhancing the analysis of current versus reference datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'advanced drift detection methods',\n",
       "  'summary_answer': 'The article mentions various advanced drift detection methods that can be customized, like PSI and K-L divergence, allowing for tailored evaluations of data drift.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_drift.mdx'},\n",
       " {'question': 'Data Summary Preset examples',\n",
       "  'summary_answer': 'The article provides code examples for using the Data Summary Preset to generate reports and visualize dataset statistics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'how to compare datasets with Data Summary',\n",
       "  'summary_answer': 'You can use the Data Summary Preset to compare two datasets side-by-side by passing both to the report run method.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'data quality tests in Data Summary',\n",
       "  'summary_answer': 'The Data Summary Preset enables auto-generated data quality tests based on either a reference dataset or heuristics, helping to ensure data integrity.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_data_summary.mdx'},\n",
       " {'question': 'basic recommender system setup',\n",
       "  'summary_answer': 'The article explains how to set up a basic recommender system using the Report and RecSysPreset classes in the Evidently API to generate top-k recommendations.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'metrics evaluation for recsys',\n",
       "  'summary_answer': 'It discusses the various metrics available in RecsysPreset for evaluating recommendation quality, including metrics like NDCG at K and diversity score.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'customizing report conditions in recsys',\n",
       "  'summary_answer': 'The article outlines methods for modifying test conditions and report composition to enhance performance evaluations in recommender systems.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_recsys.mdx'},\n",
       " {'question': 'using RegressionPreset in Python example',\n",
       "  'summary_answer': 'The article explains how to create a report using the `RegressionPreset` in Python to evaluate model performance on a dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'understanding regression quality metrics',\n",
       "  'summary_answer': 'It discusses various regression quality metrics like MAE, RMSE, and how they are used to assess model performance within the report.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'compare regression model outputs',\n",
       "  'summary_answer': 'The article outlines how to use the `RegressionPreset` to compare performance across different datasets or reference models as part of the evaluation process.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_regression.mdx'},\n",
       " {'question': 'running Text Evals report example',\n",
       "  'summary_answer': 'The article provides a code example for running a Text Evals report using the `Report` class in Python, detailing how to evaluate a current dataset.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'data requirements for Text Evals',\n",
       "  'summary_answer': 'The article highlights that the Text Evals Preset requires an input dataset with computed descriptors and allows for the comparison of one or two datasets.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'customizing Text Evals report',\n",
       "  'summary_answer': 'Customization options for the Text Evals report are discussed, including selecting specific descriptors and modifying test conditions to refine the evaluation process.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'metrics/preset_text_evals.mdx'},\n",
       " {'question': 'evidently lllm evaluation setup',\n",
       "  'summary_answer': 'The article explains how to set up your environment for LLM evaluation using Evidently, including installation instructions and necessary imports.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'how to create a report in evidently',\n",
       "  'summary_answer': 'It details the steps to create and run a report summarizing evaluation results in Evidently, including examples of generating reports in JSON or HTML formats.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'define dataset for llm evaluation',\n",
       "  'summary_answer': 'The article describes how to prepare a dataset for LLM evaluation, including examples of input and output structures that are compatible with Evidently.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'install evidently library',\n",
       "  'summary_answer': 'Instructions are provided for installing the Evidently library in Python, including usage of pip for installation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'understand llm evaluation descriptors',\n",
       "  'summary_answer': 'It offers insights into the various descriptors used in LLM evaluations, such as Sentiment and TextLength, explaining their purposes and how to implement them.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'using openai key in evidently',\n",
       "  'summary_answer': 'The article outlines how to set an OpenAI API key as an environment variable to facilitate evaluations using OpenAI models in Evidently.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'steps to analyze llm output',\n",
       "  'summary_answer': 'It walks through the entire process of analyzing LLM outputs, emphasizing the use of various checks and evaluations to ensure quality responses.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'custom llm judges implementation',\n",
       "  'summary_answer': 'Instructions are provided on how to implement custom LLM judges using built-in templates in Evidently for specific evaluation criteria.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'create dashboard for llm evaluations',\n",
       "  'summary_answer': 'The article explains how to create a dashboard in Evidently to track evaluation results over time, customizing panels for better data visualization.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'question': 'data drift evaluation Python example',\n",
       "  'summary_answer': 'The article details how to evaluate data drift in Python using Evidently, including setting up datasets and generating reports.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'how to install evidently library',\n",
       "  'summary_answer': 'Instructions for installing the Evidently library in Python are provided, including the command to use.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'what is data drift in ML',\n",
       "  'summary_answer': 'Data drift refers to changes in the data distribution over time, impacting model performance; the article explains its significance in ML evaluations.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'create project in evidently',\n",
       "  'summary_answer': 'The article guides users on creating a project in Evidently, including the necessary setup steps and code examples.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'generate ML reports in Python',\n",
       "  'summary_answer': 'You can generate ML reports in Python using the Evidently library, and the article outlines the process for running evaluations and viewing results.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'Evidently dashboard configuration',\n",
       "  'summary_answer': 'The article includes instructions on how to configure dashboards in Evidently for tracking evaluation results over time.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_ml.mdx'},\n",
       " {'question': 'setup tracing LLM app',\n",
       "  'summary_answer': 'The article outlines the steps to set up tracing for an LLM application, including installing necessary libraries and configuring the tracking environment.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'Evidently Cloud integration with tracing',\n",
       "  'summary_answer': 'It explains how to integrate Evidently Cloud for viewing and evaluating LLM traces effectively within the application setup.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'text',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'install tracely and evidently in Python',\n",
       "  'summary_answer': 'The article provides installation commands for the necessary libraries, specifically tracely and evidently, for Python users setting up LLM tracing.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'evaluate LLM responses using evidently',\n",
       "  'summary_answer': 'Instructions for evaluating LLM responses are given, including how to add descriptors and generate reports using the collected trace data.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'tracing edge cases implementation',\n",
       "  'summary_answer': 'For advanced users, the article covers how to capture edge cases during tracing and provides implementation details for specific monitoring needs.',\n",
       "  'difficulty': 'advanced',\n",
       "  'intent': 'code',\n",
       "  'filename': 'quickstart_tracing.mdx'},\n",
       " {'question': 'adversarial testing examples',\n",
       "  'summary_answer': 'The article provides insights on creating adversarial test datasets to expose AI model vulnerabilities, listing several predefined scenarios for testing, such as harmful content and forbidden topics.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/adversarial_data.mdx'},\n",
       " {'question': 'how to configure adversarial dataset',\n",
       "  'summary_answer': 'Instructions are included for configuring your own adversarial dataset in the Evidently UI, including selecting test scenarios and customizing input generation.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/adversarial_data.mdx'},\n",
       " {'question': 'generate test inputs for AI system',\n",
       "  'summary_answer': 'The article details how to create synthetic input test cases in the Evidently platform by defining scenarios and generating relevant questions, expanding test coverage for AI systems.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/input_data.mdx'},\n",
       " {'question': 'generate synthetic test data example',\n",
       "  'summary_answer': 'The article explains how to generate synthetic test inputs and outputs using Evidently Cloud to evaluate AI systems, providing practical examples and use cases for this process.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/introduction.mdx'},\n",
       " {'question': 'generate test dataset for RAG system',\n",
       "  'summary_answer': 'The article outlines steps to create a synthetic test dataset for RAG systems, guiding users through project creation, uploading knowledge bases, and refining generated test cases.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/rag_data.mdx'},\n",
       " {'question': 'when to use synthetic data for AI evaluations',\n",
       "  'summary_answer': 'Synthetic data is essential when real data is unavailable or insufficient, particularly for testing edge cases, adversarial inputs, and during the initial phases of AI development.',\n",
       "  'difficulty': 'beginner',\n",
       "  'intent': 'text',\n",
       "  'filename': 'synthetic-data/why_synthetic.mdx'},\n",
       " {'question': 'examples of generating synthetic data for tests',\n",
       "  'summary_answer': 'The article discusses how to create synthetic test datasets to enhance evaluations by quickly generating structured cases and filling gaps in data scenarios.',\n",
       "  'difficulty': 'intermediate',\n",
       "  'intent': 'code',\n",
       "  'filename': 'synthetic-data/why_synthetic.mdx'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3373fab-271c-4bf3-8a2e-bdfc48fb940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b615ae76-c162-4abb-978c-6544eb75c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "\n",
    "pricing = PricingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99643f57-50d2-4187-bcc5-10fd21672fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.026059199999999998, output_cost=0.013647000000000001, total_cost=0.0397062)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "\n",
    "for r in results:\n",
    "    usage = r['usage']\n",
    "    input_tokens = input_tokens + usage.input_tokens\n",
    "    output_tokens = output_tokens + usage.output_tokens\n",
    "    \n",
    "pricing.calculate_cost('gpt-4o-mini', input_tokens, output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce9ca4b1-69ab-4fb3-b1c8-ef6dba3a7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(final_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae0a6b96-0f1a-47c0-b7b8-c73f5c796c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "688043b3-2048-4035-b9fe-e45ad6317aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,summary_answer,difficulty,intent,filename\n",
      "mapping input data in Evidently,The article details how to use the `DataDefinition` object to map input data correctly for data evaluations in Evidently.,beginner,text,docs/library/data_definition.mdx\n",
      "define column types in DataDefinition,\"It outlines the various column types that can be defined in a `DataDefinition`, including categorical, numerical, text, and datetime columns.\",beginner,text,docs/library/data_definition.mdx\n",
      "create Dataset object in Evidently,\"To create a `Dataset` object, you can use `Dataset.from_pandas` with a specified `DataDefinition` to ensure correct data processing.\",beginner,code,docs/library/data_definition.mdx\n",
      "DataDefinition manual mapping examples,\"The article provides examples of how to manually define a `DataDefinition`, including specific columns for text, numerical, and categorical data.\",intermediate,code,docs/library/data_definition.mdx\n",
      "default column mappings in Evidently,\"It describes how automatic mapping works for different column types if no explicit mapping is given, detailing the defaults for numerical, categorical, and datetime columns.\",intermediate,text,docs/library/data_definition.mdx\n",
      "pandas.DataFrame to Dataset object,\"You can directly use a `pandas.DataFrame` with the `report.run()` method, but it's recommended to create a `Dataset` object for clarity.\",intermediate,code,docs/library/data_definition.mdx\n",
      "importance of ID and timestamp columns,The article highlights the significance of correctly identifying ID and timestamp columns for accurate evaluations and their respective roles.,beginner,text,docs/library/data_definition.mdx\n",
      "multiple regression mapping in DataDefinition,It shows how to set up regression checks by mapping target and prediction columns within the `DataDefinition` for regression scenarios.,advanced,code,docs/library/data_definition.mdx\n",
      "mapping for LLM evaluations,\"For LLM evaluations, you can specify text columns in the `DataDefinition` to ensure proper assessment of text data.\",intermediate,code,docs/library/data_definition.mdx\n"
     ]
    }
   ],
   "source": [
    "!head ground_truth_evidently.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
