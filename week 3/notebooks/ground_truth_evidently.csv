question,summary_answer,difficulty,intent,filename
mapping input data in Evidently,The article details how to use the `DataDefinition` object to map input data correctly for data evaluations in Evidently.,beginner,text,docs/library/data_definition.mdx
define column types in DataDefinition,"It outlines the various column types that can be defined in a `DataDefinition`, including categorical, numerical, text, and datetime columns.",beginner,text,docs/library/data_definition.mdx
create Dataset object in Evidently,"To create a `Dataset` object, you can use `Dataset.from_pandas` with a specified `DataDefinition` to ensure correct data processing.",beginner,code,docs/library/data_definition.mdx
DataDefinition manual mapping examples,"The article provides examples of how to manually define a `DataDefinition`, including specific columns for text, numerical, and categorical data.",intermediate,code,docs/library/data_definition.mdx
default column mappings in Evidently,"It describes how automatic mapping works for different column types if no explicit mapping is given, detailing the defaults for numerical, categorical, and datetime columns.",intermediate,text,docs/library/data_definition.mdx
pandas.DataFrame to Dataset object,"You can directly use a `pandas.DataFrame` with the `report.run()` method, but it's recommended to create a `Dataset` object for clarity.",intermediate,code,docs/library/data_definition.mdx
importance of ID and timestamp columns,The article highlights the significance of correctly identifying ID and timestamp columns for accurate evaluations and their respective roles.,beginner,text,docs/library/data_definition.mdx
multiple regression mapping in DataDefinition,It shows how to set up regression checks by mapping target and prediction columns within the `DataDefinition` for regression scenarios.,advanced,code,docs/library/data_definition.mdx
mapping for LLM evaluations,"For LLM evaluations, you can specify text columns in the `DataDefinition` to ensure proper assessment of text data.",intermediate,code,docs/library/data_definition.mdx
binary vs multiclass classification mapping,The article discusses the differences in mapping for binary versus multiclass classification scenarios within the `DataDefinition` framework.,advanced,text,docs/library/data_definition.mdx
automated vs manual column mapping,It explains the pros and cons of automated column mapping versus manual mapping for ensuring accurate data evaluations in Evidently.,intermediate,text,docs/library/data_definition.mdx
how to evaluate text data with descriptors,"Descriptors are used to compute scores or labels for text data evaluations, allowing for both built-in and custom evaluations across various text statistics.",beginner,text,docs/library/descriptors.mdx
using python to create descriptors,The article provides step-by-step Python code to create and use descriptors within a Dataset for evaluating text data.,beginner,code,docs/library/descriptors.mdx
custom descriptors in text evaluation,You can create custom descriptors using LLM prompts or Python functions to tailor evaluations to specific needs.,intermediate,text,docs/library/descriptors.mdx
example of descriptors for sentiment evaluation,"The text includes examples that define sentiment descriptors for evaluating responses in a dataset, showing how to check for sentiments in responses.",intermediate,code,docs/library/descriptors.mdx
available built-in descriptors,"The article lists several built-in descriptors like Sentiment, TextLength, and IncludesWords for evaluating various aspects of text data.",beginner,text,docs/library/descriptors.mdx
how to combine multiple descriptors,You can combine multiple descriptors in a single Dataset by passing them as a list to enhance evaluation results across different text features.,intermediate,code,docs/library/descriptors.mdx
Python imports for descriptors,"The guide outlines necessary imports from Evidently for implementing descriptors in text evaluations, ensuring proper utilization of the library's functionalities.",beginner,code,docs/library/descriptors.mdx
checking conditions with descriptor tests,"Descriptor Tests allow defining pass/fail checks for evaluations, enabling the assessment of conditions like text length or sentiment positivity for each row.",intermediate,code,docs/library/descriptors.mdx
generate toy data for testing descriptors,The article provides a code snippet to create sample data for testing the evaluation process using descriptors in Python.,beginner,code,docs/library/descriptors.mdx
exporting results from descriptors,You can preview and export the results of evaluations as a DataFrame or in various formats such as HTML and JSON for further analysis.,intermediate,code,docs/library/descriptors.mdx
advanced testing in dataset-level reports,The article discusses integrating tests into dataset-level metrics to generate clear pass/fail results for comprehensive evaluations of text characteristics.,advanced,code,docs/library/descriptors.mdx
configuring text evaluation reports,Custom reports can be tailored to use specific descriptors or metrics for more focused evaluation results in text datasets.,advanced,text,docs/library/descriptors.mdx
evidently evaluation workflow steps,"The article outlines the core steps of the evaluation workflow with Evidently, including data preparation, creating a Dataset object, and running evaluations.",beginner,text,docs/library/evaluations_overview.mdx
how to create a Dataset in evidently,"To create a Dataset in Evidently, you use the `Dataset.from_pandas()` method along with a `DataDefinition()` specifying column roles and types.",intermediate,code,docs/library/evaluations_overview.mdx
adding metadata to evidently evaluation results,"You can enhance your evaluation results in Evidently by adding tags or metadata to identify specific runs, which helps in organizing evaluations.",advanced,code,docs/library/evaluations_overview.mdx
generate multiple metrics at once,The article explains how to use metric generator helper functions to generate multiple metrics for dataset columns in a streamlined manner.,beginner,text,docs/library/metric_generator.mdx
ColumnMetricGenerator example code,It provides examples of using the ColumnMetricGenerator to apply metrics like ValueDrift to columns in datasets with Python code snippets.,intermediate,code,docs/library/metric_generator.mdx
exporting reports in JSON format,"The article explains how to export evaluation results as JSON using the `my_report.json()` method, allowing easy storage and export of results.",beginner,code,docs/library/output_formats.mdx
Evidently library features,"The Evidently library offers various features for AI/ML evaluations, synthetic data generation, prompt optimization, and a visualization UI to track evaluations over time.",beginner,text,docs/library/overview.mdx
AI/ML evaluations in Evidently,"Evidently provides over 100 built-in metrics and checks to run evaluations on AI systems, offering results in formats like JSON, DataFrames, and visual reports.",beginner,code,docs/library/overview.mdx
Generate visual reports Evidently,"You can generate visual reports in formats compatible with Jupyter, Colab, or as HTML files, which display multiple metrics and evaluation results.",beginner,code,docs/library/overview.mdx
Synthetic data generation Evidently,"Evidently helps generate structured synthetic data primarily designed for LLM use cases, allowing for the creation of test datasets for AI applications.",beginner,code,docs/library/overview.mdx
Prompt optimization tools,"The Evidently library includes tools for optimizing prompts using evaluation capabilities, which can help users generate better prompts based on feedback and targets.",beginner,code,docs/library/overview.mdx
Setup tracking and visualization UI,"Evidently offers a lightweight UI for tracking and visualizing evaluation results over time, allowing users to store and compare evaluations.",beginner,code,docs/library/overview.mdx
Export metrics and reports,"You can export evaluation scores as JSON, pandas DataFrames, or generate visual reports, facilitating integration into various workflows.",beginner,code,docs/library/overview.mdx
Descriptors in Evidently library,"Descriptors provide row-level assessments for specific qualities of text, enabling detailed evaluations of outputs from AI systems.",intermediate,text,docs/library/overview.mdx
Creating a Dataset object in Evidently,"To run evaluations, you must create a Dataset object in Evidently, which allows you to map data columns and attach meta-information for processing.",intermediate,code,docs/library/overview.mdx
Evaluating classification quality,"Evidently requires specific columns for evaluating classification quality, mapping actual and predicted labels for insightful comparisons.",intermediate,code,docs/library/overview.mdx
Data drift detection methods,Evidently includes over 20 tests and metrics to detect data drift by comparing current datasets with reference datasets to identify distribution changes.,intermediate,text,docs/library/overview.mdx
Reports and metrics functionality,"Reports in Evidently allow users to summarize and analyze datasets, running evaluations across various metrics and providing visual summaries.",intermediate,code,docs/library/overview.mdx
Conditional tests in Evidently,"Tests in Evidently validate results against specific expectations, allowing users to set conditions for metrics to ensure quality and performance.",intermediate,code,docs/library/overview.mdx
Using metric presets,"Evidently offers metric presets that are pre-configured templates for specific evaluation scenarios, streamlining the process of running evaluations.",intermediate,code,docs/library/overview.mdx
Benefits of using Test Suites,"Test Suites in Evidently allow users to run multiple tests simultaneously, offering a summary of outcomes for effective monitoring and debugging.",intermediate,text,docs/library/overview.mdx
Automating evaluations with Evidently,"You can integrate Evidently's evaluation capabilities into automation workflows, triggering actions based on test results within your data pipeline.",advanced,code,docs/library/overview.mdx
Edge cases in dataset evaluations,Advanced users can explore various edge cases when evaluating datasets in Evidently by customizing tests and metrics according to specific scenarios.,advanced,code,docs/library/overview.mdx
Comparing datasets in evaluations,"Evidently allows comparisons between two datasets, which is useful for side-by-side evaluations or detecting data drift over time.",advanced,text,docs/library/overview.mdx
Handling large datasets in Evidently,"When working with large datasets, it is advisable to apply sampling methods to optimize performance during evaluations in the Evidently library.",advanced,text,docs/library/overview.mdx
generate reports with Evidently,"This article explains how to use the Evidently library to generate reports, including step-by-step instructions and code examples.",beginner,code,docs/library/report.mdx
Evidently report presets,"The article discusses the use of Metric Presets in Evidently to create reports that are ready to use out of the box, along with examples of generating specific reports.",intermediate,code,docs/library/report.mdx
Data Summary Report example,"An example of generating a Data Summary Report using Evidently is provided, demonstrating how to create and run a report on a single dataset.",beginner,code,docs/library/report.mdx
compare results in Evidently reports,"The article details how to compare multiple report snapshots side-by-side using the compare function in Evidently, aiding in result analysis.",advanced,code,docs/library/report.mdx
how to add metadata to Evidently reports,"The article explains how to incorporate metadata into reports by using Python dictionaries for key:value pairs, enhancing report categorization and filtering capabilities.",beginner,code,docs/library/tags_metadata.mdx
custom tags and metadata examples Evidently,"The article includes code examples showing how to create custom tags and metadata for reports, allowing users to tailor their evaluations based on specific criteria or contexts.",intermediate,code,docs/library/tags_metadata.mdx
how to import testing modules evidently,"To use Tests, import the following modules: from evidently import Report, from evidently.metrics import *, from evidently.presets import *, from evidently.tests import *.",beginner,code,docs/library/tests.mdx
types of tests in evidently,"The article discusses three ways to implement Tests: Tests Presets, Tests with defaults, and Custom Tests, allowing for flexible data validation.",beginner,text,docs/library/tests.mdx
example of using test presets evidently,"You can enable Test Presets by setting include_tests=True in the Report, which automatically generates a suite of Tests for your data evaluation.",intermediate,code,docs/library/tests.mdx
custom test conditions examples evidently,"You can define specific pass/fail conditions for each Test by using parameters like gt, lt, eq while creating a Report instance.",intermediate,code,docs/library/tests.mdx
using reference dataset for tests evidently,"When using a reference dataset, Tests will compare new data against it to validate conditions, such as checking for missing values.",intermediate,code,docs/library/tests.mdx
difference between include_tests True and False,"Setting include_tests=True adds Tests to your Report while False excludes them, allowing flexibility in what validations to include.",intermediate,text,docs/library/tests.mdx
how to run personalized tests evidently,"You can run personalized tests by selecting specific Metrics and setting conditions manually, giving you control over the validation process.",beginner,code,docs/library/tests.mdx
setting warning vs fail for tests evidently,"You can configure Tests to return a Warning instead of a Fail by setting is_critical=False, helping to manage alert fatigue.",advanced,code,docs/library/tests.mdx
conditions for testing share vs absolute evidently,"The article explains how to set conditions for testing both share and absolute values, using parameters like tests for absolute and share_tests for relative comparisons.",advanced,text,docs/library/tests.mdx
set up alerts in Evidently,"The article provides a step-by-step guide on setting up alerts in Evidently, including choosing notification channels like Email or Slack and defining alert conditions based on test failures or custom metric values.",beginner,code,docs/platform/alerts.mdx
add panel to dashboard api example,"The article provides code examples on how to add panels to a dashboard using the Python API, including multiple types of panels.",beginner,code,docs/platform/dashboard_add_panels.mdx
dashboard management api,"It details how to manage dashboards programmatically, including adding and deleting tabs and panels using the Python API.",beginner,text,docs/platform/dashboard_add_panels.mdx
evidently dashboard panels,The article explains how to utilize Evidently's API to create and customize dashboard panels for reporting purposes.,beginner,text,docs/platform/dashboard_add_panels.mdx
create new tab in dashboard api,The article specifies how to use the API to create new tabs in a dashboard with the example code provided for `add_tab`.,beginner,code,docs/platform/dashboard_add_panels.mdx
delete panel from dashboard,"Code snippets are provided to delete specific panels from the dashboard, helping users manage their dashboard effectively.",intermediate,code,docs/platform/dashboard_add_panels.mdx
python api clear dashboard example,This article explains how to clear all panels and tabs from a dashboard using the `clear_dashboard` method in Python.,intermediate,code,docs/platform/dashboard_add_panels.mdx
dashboard panel plot types,"It describes different types of panels that can be added, such as text, counters, and plots including bar and line charts.",beginner,text,docs/platform/dashboard_add_panels.mdx
add pie chart to dashboard,There are examples in the article on how to add pie chart panels to a dashboard using various metrics.,intermediate,code,docs/platform/dashboard_add_panels.mdx
panel metric options,"The article summarizes the parameters available for configuring `PanelMetric`, guiding how to specify what to display.",intermediate,text,docs/platform/dashboard_add_panels.mdx
multiple values in dashboard panel,"It provides an example of how to display multiple values in a single panel, particularly with line charts.",advanced,code,docs/platform/dashboard_add_panels.mdx
evidently metrics for dashboard,The article elaborates on how to reference and utilize metrics from Evidently in dashboard panels effectively.,intermediate,text,docs/platform/dashboard_add_panels.mdx
error handling when adding panels api,Users are guided on how to handle potential errors when trying to add panels or tabs that do not exist.,advanced,text,docs/platform/dashboard_add_panels.mdx
api dashboard panel visualization settings,"The article discusses visualization settings for dashboard panels, including how to configure the plot types and sizes.",beginner,text,docs/platform/dashboard_add_panels.mdx
create custom panels in dashboard,"The article explains how to create custom panels in a dashboard by entering 'Edit' mode, clicking 'Add Panel', and following prompts to configure it with relevant metrics.",beginner,code,docs/platform/dashboard_add_panels_ui.mdx
adding tabs in dashboard,"It details the steps to add tabs in a dashboard, including entering 'Edit' mode and using the 'add Tab' option to organize panels.",beginner,code,docs/platform/dashboard_add_panels_ui.mdx
using pre-built dashboard tabs,The article describes how to use pre-built tabs which come with preset panel combinations and rely on having the necessary metrics available.,intermediate,text,docs/platform/dashboard_add_panels_ui.mdx
panel configuration in dashboard,"It provides advanced tips for configuring panels, such as filtering metrics by tags and adjusting plot types, to effectively display data.",advanced,code,docs/platform/dashboard_add_panels_ui.mdx
Dashboard features in Evidently,"The article explains the main features of the Dashboard in Evidently, which allows users to track AI application performance through evaluations and live data quality monitoring.",beginner,text,docs/platform/dashboard_overview.mdx
How to add Panels to Dashboard using Python API,"It details the process of adding Panels to the Dashboard using the Python API, including the necessary parameters and customization options for displaying metrics.",intermediate,code,docs/platform/dashboard_overview.mdx
create dataset in Evidently OSS,"The article explains various methods to create datasets in Evidently OSS, including uploading CSV files through the UI or Python API, and generating synthetic data.",beginner,code,docs/platform/datasets_overview.mdx
what is synthetic data in Evidently,"The article defines synthetic data as datasets generated directly in Evidently Cloud, with options to create them from examples or source documents, highlighting their importance in evaluations.",intermediate,text,docs/platform/datasets_overview.mdx
upload dataset to Evidently,The article details how to prepare and upload a dataset to an Evidently project using the `add_dataset` method in Python or through the UI.,beginner,code,docs/platform/datasets_workflow.mdx
how to include dataset in Reports Evidently,"It explains how to include datasets when uploading reports, using the `include_data` parameter to simultaneously upload the report and associated dataset.",intermediate,code,docs/platform/datasets_workflow.mdx
run evals with Evidently API,"The article provides a guide on using the Evidently API to run evaluations, detailing a simple code example for executing a text evaluation and logging it on the platform.",beginner,code,docs/platform/evals_api.mdx
Evidently eval workflow steps,"The document outlines the complete workflow for running evals, including steps like configuring a report, uploading results, exploring outcomes, and optionally setting up dashboards and alerts.",intermediate,text,docs/platform/evals_api.mdx
view evaluation results on platform,The article explains how to access and explore evaluation results by navigating to the Reports section of your project and using the Explore view to analyze metrics and datasets.,beginner,code,docs/platform/evals_explore.mdx
no code data evaluation steps,"The article outlines steps for evaluating data without coding, including dataset preparation, adding descriptors, and running evaluations using various methods.",beginner,text,docs/platform/evals_no_code.mdx
how to prepare dataset for no code evals,"To prepare a dataset in a no-code interface, you can either upload a CSV file or use an existing dataset from the platform.",beginner,code,docs/platform/evals_no_code.mdx
add descriptors in no code evaluations,"You can add descriptors to your dataset evaluation through the user interface, allowing you to choose evaluation methods and specify checks.",intermediate,code,docs/platform/evals_no_code.mdx
using LLM for no code evaluations,"The article explains how to integrate an LLM provider API key for evaluations, allowing you to classify or score texts directly using LLM models.",advanced,text,docs/platform/evals_no_code.mdx
evals workflow stages AI product,"The article outlines different evaluation stages in AI product development, including ad hoc analysis, experiments, and monitoring, highlighting the importance of each step.",beginner,text,docs/platform/evals_overview.mdx
run evals locally Evidently API examples,"The article explains how to execute evaluations via the Evidently API, offering Python-based workflows for experiments and CI/CD integrations.",intermediate,code,docs/platform/evals_overview.mdx
batch evaluation job example in Evidently,"The article provides a simple code example demonstrating how to run a batch evaluation job using the Dataset and Report classes from the Evidently library, illustrating how to upload dataset stats to the workspace.",beginner,code,docs/platform/monitoring_local_batch.mdx
Evidently batch monitoring workflow steps,"The article outlines the complete workflow for batch monitoring, including steps for configuring metrics, running evaluations, uploading results, and setting up dashboards and alerts.",intermediate,text,docs/platform/monitoring_local_batch.mdx
AI quality monitoring overview,"The article explains AI observability, detailing how quality monitoring facilitates evaluating inputs and outputs of AI applications in production.",beginner,text,docs/platform/monitoring_overview.mdx
batch monitoring setup with Evidently,"It describes how to set up batch monitoring jobs using Evidently, including building evaluation pipelines and running metric calculations.",intermediate,code,docs/platform/monitoring_overview.mdx
tracing with scheduled evaluations,The article outlines how to implement tracing in LLM-powered applications by capturing relevant data and scheduling evaluations using the Evidently platform.,advanced,code,docs/platform/monitoring_overview.mdx
Evidently Platform features overview,"The article summarizes key features of the Evidently Platform, including various evaluation methods, dataset management, synthetic data generation, and monitoring capabilities.",beginner,text,docs/platform/overview.mdx
Using Evidently Python library for evaluations,"The article describes how to run evaluations locally with the Evidently Python library, highlighting no-code options and tracking capabilities for experiments.",intermediate,code,docs/platform/overview.mdx
create project in Evidently,"To create a project, you can either use Python code or the UI, with options for specifying an organization when using the cloud.",beginner,code,docs/platform/projects_manage.mdx
how to connect to an existing project,You can connect to an existing project in Python using the `get_project` method along with the project ID.,intermediate,code,docs/platform/projects_manage.mdx
delete project using UI,"To delete a project using the UI, hover over the project and click the 'delete' button, but note that this action removes all associated data.",beginner,code,docs/platform/projects_manage.mdx
project parameters explained,"The article lists the parameters associated with a project, including name, ID, description, and dashboard configuration, providing a clear understanding of their roles.",advanced,text,docs/platform/projects_manage.mdx
how to organize projects in Evidently,"The article explains how to structure Projects in Evidently based on various criteria like application, model, test scenario, phase, or use case to optimize data management and evaluation.",beginner,text,docs/platform/projects_overview.mdx
LLM tracing overview,"The article defines LLM tracing as a method to instrument AI applications for detailed data collection on their operations, highlighting its importance for performance evaluation and analysis.",beginner,text,docs/platform/tracing_overview.mdx
install tracely package example,The article provides a quick installation command for the `tracely` package using pip: `pip install tracely`.,beginner,code,docs/platform/tracing_setup.mdx
how to initialize tracing with tracely,"To initialize tracing, use the `init_tracing` function with parameters such as `address`, `api_key`, and `project_id` as described in the article.",beginner,code,docs/platform/tracing_setup.mdx
tracing function arguments in tracely,"The article outlines the parameters for the `init_tracing()` function, including `address`, `api_key`, `project_id`, and their corresponding environment variables.",intermediate,text,docs/platform/tracing_setup.mdx
use trace_event decorator in tracely,"You can use the `@trace_event` decorator to collect traces for specific functions in your application, as explained with examples in the article.",beginner,code,docs/platform/tracing_setup.mdx
environment variables for tracely,The article explains how to set parameters like `api_key` and `export_name` using environment variables which correspond to the function arguments.,intermediate,text,docs/platform/tracing_setup.mdx
get export_id for tracing dataset,"To retrieve the `export_id` of your tracing dataset, use the `get_info()` function provided in the `tracely` package as mentioned in the article.",beginner,code,docs/platform/tracing_setup.mdx
creating nested events with tracely,"You can trace multi-step workflows by using nested `@trace_event` decorators, where each function will automatically appear as a child span of the parent trace, as described in the article.",intermediate,code,docs/platform/tracing_setup.mdx
context manager for tracing in tracely,"The article describes how to use a context manager to create trace events, allowing for more control over tracing specific blocks of code without decorators.",intermediate,code,docs/platform/tracing_setup.mdx
set attributes in span using tracely,"You can add attributes to the current span using the `get_current_span()` method to access the active span and set attributes, as shown in the article.",intermediate,code,docs/platform/tracing_setup.mdx
linking events across different systems tracely,"The article explains how to connect events across systems into a single trace using the `tracely.bind_to_trace` method, allowing for complex tracing scenarios.",advanced,text,docs/platform/tracing_setup.mdx
setup Evidently Cloud account steps,"The article outlines how to create an Evidently Cloud account, generate an API token, and connect from Python, providing detailed instructions for beginners.",beginner,code,docs/setup/cloud.mdx
self-host Evidently UI service setup,"The article explains the steps to set up the Evidently UI service for self-hosting, including creating workspaces and launching the service.",beginner,code,docs/setup/self-hosting.mdx
how to create a workspace for Evidently,"It details the process of creating a local or remote workspace to store evaluation results, necessary for utilizing the UI service effectively.",beginner,code,docs/setup/self-hosting.mdx
Evidently UI local vs remote workspace,"The article compares local and remote workspaces, explaining the differences in setup and data storage options for the Evidently UI service.",intermediate,text,docs/setup/self-hosting.mdx
launch Evidently UI service command,Instructions are provided on the terminal commands required to launch the Evidently UI service depending on workspace configurations.,beginner,code,docs/setup/self-hosting.mdx
Evidently UI delete workspace command,"The article warns about the consequences of deleting a workspace and provides the necessary command to do so, emphasizing data loss.",intermediate,code,docs/setup/self-hosting.mdx
automated testing LLM outputs GitHub Actions,"The article explains how to set up automated testing of LLM outputs using Evidently in GitHub Actions, detailing dataset definitions, evaluation methods, and reporting results in CI.",beginner,code,examples/GitHub_actions.mdx
LLM evaluation methods overview,"The article outlines different evaluation methods for LLMs, including API setup and various metrics used for assessment.",beginner,text,examples/LLM_evals.mdx
reference-free evaluation examples,"The article discusses reference-free evaluation techniques such as text statistics and ML models, providing code examples for implementation.",intermediate,code,examples/LLM_evals.mdx
evaluate text with LLM judge,"The article explains how to use an LLM as a judge for evaluating text based on reference and custom criteria, providing coding examples for implementation.",beginner,code,examples/LLM_judge.mdx
setting up LLM evaluator,"Instructions are included on how to set up and run an LLM evaluator, starting with prompt design and dataset creation.",beginner,code,examples/LLM_judge.mdx
create evaluation dataset for LLM,"You can create a toy dataset to evaluate the LLM judge's performance by defining questions, target responses, and manual labels for accuracy assessment.",beginner,code,examples/LLM_judge.mdx
openai API key for LLM,The article details how to pass your OpenAI API key as an environment variable to access the LLM evaluator.,beginner,code,examples/LLM_judge.mdx
reference-based evaluator examples,"Examples of creating a reference-based evaluator using the LLM judge are presented, which involves comparing new responses against predefined correct answers.",intermediate,code,examples/LLM_judge.mdx
how to score data with LLM judge,The tutorial describes how to score your data using the LLM judge by integrating evaluation descriptors into your dataset.,intermediate,code,examples/LLM_judge.mdx
custom criteria for LLM evaluation,"You can define custom evaluation criteria for the LLM judge to assess outputs when no direct reference is available, enhancing flexibility.",intermediate,text,examples/LLM_judge.mdx
installing Evidently for LLM,"Installation instructions for the Evidently package needed to run the LLM judge are provided, including the pip command.",beginner,code,examples/LLM_judge.mdx
preview LLM judge results,The article explains how to preview results of the LLM judge using Pandas DataFrame in Python after running evaluations.,intermediate,code,examples/LLM_judge.mdx
LLM judge accuracy evaluation,"The tutorial assesses the accuracy of the LLM judge by comparing its predictions with manual labels, providing insights into performance.",advanced,text,examples/LLM_judge.mdx
verbosity evaluator setup,"Instructions for setting up a verbosity evaluator with the LLM judge are included, focusing on checking response conciseness.",intermediate,code,examples/LLM_judge.mdx
report generation for LLM evaluations,You will learn to generate reports summarizing the evaluation results of the LLM judge and how to visualize them using Evidently.,intermediate,code,examples/LLM_judge.mdx
create a toy Q&A dataset,"Steps to create a toy Q&A dataset for testing the LLM judge are detailed, including sample questions and responses.",beginner,code,examples/LLM_judge.mdx
using LLMEval for judging,"The article guides you on utilizing LLMEval to implement evaluations with the LLM judge, providing a practical template.",intermediate,code,examples/LLM_judge.mdx
integrating LLM judge into workflows,Tips are provided on integrating the LLM judge into evaluation workflows to enhance testing of prompts and outputs.,advanced,text,examples/LLM_judge.mdx
descriptors for LLM evaluation,You will learn about adding evaluation descriptors to datasets for enhanced analysis of LLM predictions in your model.,intermediate,code,examples/LLM_judge.mdx
evidently cloud platform integration,Information on how to connect your LLM evaluations with the Evidently Cloud platform for better tracking and reporting of results is presented.,intermediate,code,examples/LLM_judge.mdx
managing evaluation datasets,The tutorial teaches how to manage and explore your evaluation datasets systematically within the context of LLM judging.,advanced,text,examples/LLM_judge.mdx
analyzing confusion matrix for LLM performance,The article includes details on analyzing the confusion matrix to evaluate the performance of the LLM judge further.,advanced,text,examples/LLM_judge.mdx
is LLM judge reliable?,"Insights into the reliability and accuracy of the LLM judge are shared, along with metrics to assess its performance.",advanced,text,examples/LLM_judge.mdx
LLM evaluation approach,The article details an evaluation method using multiple LLMs to assess outputs and provides guidance on how to aggregate results or uncover disagreements.,beginner,text,examples/LLM_jury.mdx
how to set up LLM judges,It explains how to configure evaluator LLMs by passing API keys and creating a project in the Evidently Cloud workspace.,beginner,code,examples/LLM_jury.mdx
examples of email evaluation,The article provides toy datasets with user intents and model-generated emails used for judging the appropriateness of email content.,intermediate,code,examples/LLM_jury.mdx
installing Evidently,It outlines the installation process for Evidently and other necessary components needed for LLM evaluation.,beginner,code,examples/LLM_jury.mdx
criteria for evaluating emails,The criteria for what constitutes an appropriate email are defined using the `BinaryClassificationPromptTemplate` in the code example.,intermediate,code,examples/LLM_jury.mdx
steps to create LLM evaluation report,"The article provides detailed steps for running evaluations, exporting data, and generating summary reports for LLM judgments.",intermediate,code,examples/LLM_jury.mdx
understanding judge disagreements,The article describes how to implement custom descriptors to flag disagreements between LLM assessments in email evaluations.,advanced,code,examples/LLM_jury.mdx
Evidently Cloud workspace setup,It discusses optional configuration of an Evidently Cloud workspace for storing and exploring evaluation results.,intermediate,code,examples/LLM_jury.mdx
aggregate LLM outputs,The article explains how to aggregate outputs from multiple LLMs to determine a consensus on email appropriateness.,beginner,text,examples/LLM_jury.mdx
RAG system evaluation metrics,"The article explains the metrics used to evaluate RAG systems, focusing on both retrieval and generation quality, and provides code examples for implementation.",beginner,text,examples/LLM_rag_evals.mdx
how to install Evidently for RAG,"Instructions for installing Evidently, including the necessary import statements and setting the OpenAI key, are provided in the tutorial.",beginner,code,examples/LLM_rag_evals.mdx
multiple context retrieval RAG evaluation,"The article discusses how to evaluate the relevance of multiple contexts retrieved by a RAG system, with examples of how to implement this using Evidently.",intermediate,code,examples/LLM_rag_evals.mdx
evaluate context quality in RAG,"It shows how to assess overall context quality for single and multiple contexts using descriptors in Evidently, with code snippets provided for clarity.",intermediate,code,examples/LLM_rag_evals.mdx
generate structured reports for RAG,"The tutorial describes how to generate structured reports in Evidently to show RAG performance evaluation results, including code snippets for report creation.",beginner,code,examples/LLM_rag_evals.mdx
what are ground truth datasets in RAG,"The article explains the concept of ground truth datasets, how they are used in evaluating the RAG system, and methods to compare generated outputs against them.",intermediate,text,examples/LLM_rag_evals.mdx
faithfulness evaluation in RAG,"Instructions on evaluating faithfulness of generated responses in RAG systems are provided, along with code examples for this evaluation method.",intermediate,code,examples/LLM_rag_evals.mdx
relevance score aggregation methods,"The article discusses different aggregation methods for scoring relevance in RAG evaluations, comparing methods like hit rate and mean relevance with practical coding examples.",advanced,code,examples/LLM_rag_evals.mdx
using CorrectnessLLMEval in RAG,"It details how to use CorrectnessLLMEval for assessing generated responses in RAG evaluations, providing necessary code snippets for implementation.",intermediate,code,examples/LLM_rag_evals.mdx
how to upload RAG evaluation results,"The tutorial explains the procedure for uploading evaluation results to the Evidently Cloud platform, enhancing tracking and simplicity in sharing results.",beginner,code,examples/LLM_rag_evals.mdx
Evidently Cloud project setup,"Instructions on setting up a project in Evidently Cloud for RAG evaluations are provided, helping users manage their projects efficiently.",beginner,code,examples/LLM_rag_evals.mdx
overview of RAG evaluation methods,"The article provides an overview of various evaluation methods for RAG systems, highlighting their importance and practical implementation through code examples.",beginner,text,examples/LLM_rag_evals.mdx
customizing LLM descriptor prompts,"Instructions are provided on how to customize prompts for LLM descriptors in RAG evaluations, enhancing the specificity of evaluations in Evidently.",advanced,code,examples/LLM_rag_evals.mdx
regression testing LLM outputs,The article explains the method of comparing new and old outputs from LLM to ensure consistency and correctness through regression testing.,beginner,text,examples/LLM_regression_testing.mdx
create toy dataset for LLM,Learn how to create a toy dataset with questions and reference answers as a starting point for regression testing LLM outputs.,beginner,code,examples/LLM_regression_testing.mdx
Evidently Cloud setup for LLM,Instructions are provided for setting up and connecting to Evidently Cloud for running tests and tracking results of LLM outputs.,beginner,code,examples/LLM_regression_testing.mdx
analyze LLM output changes,The article covers techniques to analyze changes in LLM outputs by running evaluations and comparing responses to previous outputs.,intermediate,text,examples/LLM_regression_testing.mdx
import required Python modules for LLM testing,"To set up the regression testing environment, the article lists the necessary Python imports for using Evidently and LLM evaluation.",beginner,code,examples/LLM_regression_testing.mdx
define correctness judge for LLM,The article demonstrates how to implement a correctness judge using a binary classification template to evaluate LLM responses against reference answers.,intermediate,code,examples/LLM_regression_testing.mdx
maximum length check for LLM responses,Details are provided on how to set up a maximum length check to ensure LLM responses do not exceed a defined character limit.,intermediate,code,examples/LLM_regression_testing.mdx
view regression test reports,Learn how to generate and view reports on regression tests conducted on LLM outputs to assess performance and correctness over time.,intermediate,text,examples/LLM_regression_testing.mdx
run tests with LLM evaluator,Guidance is given on running tests using an LLM evaluator to check for correctness and style consistency in new responses generated from prompts.,intermediate,code,examples/LLM_regression_testing.mdx
monitoring dashboards for LLM testing,The article offers instructions on how to create monitoring dashboards to visualize LLM test results and track performance over time.,beginner,code,examples/LLM_regression_testing.mdx
install Evidently for LLM,The article includes the command to install the Evidently library to support regression testing for LLM outputs.,beginner,code,examples/LLM_regression_testing.mdx
add descriptive statistics for LLM output,It discusses adding descriptive statistics to enhance the understanding of LLM output performance using the Evidently framework.,intermediate,code,examples/LLM_regression_testing.mdx
initialize project in Evidently Cloud,Instructions for initializing a new project in Evidently Cloud are provided for tracking regression test results.,beginner,code,examples/LLM_regression_testing.mdx
use LLM judge for style checks,The article explains how to implement a style judge to evaluate the stylistic consistency of LLM responses.,advanced,code,examples/LLM_regression_testing.mdx
test LLM outputs with custom criteria,Learn about creating custom test criteria using the LLM-as-a-judge approach to evaluate the generated outputs more effectively.,advanced,code,examples/LLM_regression_testing.mdx
evaluate dataset against benchmarks,The article outlines methods to evaluate new LLM output datasets against established benchmarks for consistency and correctness.,intermediate,text,examples/LLM_regression_testing.mdx
add tags to LLM test reports,Instructions are given on how to use tags in test reports to associate specific runs with particular parameters or versions.,intermediate,code,examples/LLM_regression_testing.mdx
generate test results summary,The process of generating a summary of test results across multiple runs to assess improvements or regressions in LLM outputs is explained.,intermediate,text,examples/LLM_regression_testing.mdx
set up CI/CD for LLM testing,The article provides insights on how to integrate LLM testing into CI/CD workflows to automate evaluation on every change.,advanced,text,examples/LLM_regression_testing.mdx
create automated dashboards for LLM test results,Learn how to create automated dashboards that update with new test results for continuous monitoring of LLM output validity.,advanced,code,examples/LLM_regression_testing.mdx
LLM evaluation quickstart,"The article provides a quickstart guide that outlines the initial steps to evaluate LLM quality, suitable for new users.",beginner,code,examples/introduction.mdx
ML data drift evaluation steps,It includes a quickstart for testing tabular data quality and data drift for beginners to understand key concepts.,beginner,code,examples/introduction.mdx
How to evaluate text outputs LLM,There are end-to-end examples demonstrating how to evaluate the quality of text outputs from large language models.,beginner,code,examples/introduction.mdx
LLM judge tutorial,"The article features a tutorial on creating and evaluating an LLM judge using human labels, ideal for those new to the topic.",beginner,code,examples/introduction.mdx
Using multiple LLMs to evaluate outputs,"A tutorial demonstrates how to employ multiple LLMs as judges to assess the same output, suitable for intermediate learners.",intermediate,code,examples/introduction.mdx
LLM evaluation methods explained,"It walks through various evaluation methods, offering insights into reference-based and reference-free evaluations of LLM outputs.",intermediate,text,examples/introduction.mdx
GitHub actions for LLM integration,The article details how to integrate Evidently evaluations within GitHub actions for automated testing in CI/CD workflows.,intermediate,code,examples/introduction.mdx
Adversarial testing for LLMs,"A tutorial highlights how to conduct scenario-based adversarial testing on LLMs, focusing on brand risks.",intermediate,code,examples/introduction.mdx
Optimizing LLM judge prompts,"It explains how to optimize prompts for multi-class and binary classifiers using target labels and free-form feedback, providing practical code examples.",intermediate,code,examples/introduction.mdx
Overview of metrics in ML,"The article features various metrics used in machine learning, including regression and classification metrics, laid out in an easy-to-follow format.",beginner,text,examples/introduction.mdx
Framework for RAG evaluations,"It gives a detailed breakdown of RAG evaluations, including methods to evaluate retrieval and generative quality.",advanced,text,examples/introduction.mdx
Evidently Cloud v2 updates and changes,"The article details significant improvements in Evidently Cloud v2, including a redesigned dashboard and performance enhancements, along with breaking changes that require users to adapt their SDK version.",beginner,text,faq/cloud_v2.mdx
migrating to Evidently 0.6 changes,"The guide details key changes introduced in Evidently 0.6, including a new API and the introduction of the Report object.",beginner,text,faq/migration.mdx
new API in Evidently 0.6,"Version 0.6 features a new core API where you need to import components from evidently.future, significantly changing how reports and datasets are handled.",beginner,code,faq/migration.mdx
old API compatibility Evidently 0.6 to 0.6.7,"During the transition from version 0.6 to 0.6.7, both the new and legacy APIs coexist, allowing for flexibility in migrating to the updated system.",intermediate,text,faq/migration.mdx
how to create Dataset in Evidently,"You now need to explicitly create a Dataset object in the new Evidently API, replacing the previous method of directly passing a DataFrame.",intermediate,code,faq/migration.mdx
using descriptors in new Evidently version,"The article explains how descriptors have been updated, allowing you to perform evaluations and checks in a more modular approach with improved APIs.",intermediate,code,faq/migration.mdx
Reports API changes Evidently 0.6,"The Reports API has undergone significant changes including the unification of Reports and Tests, simplifying the way metrics and evaluations are managed together.",advanced,text,faq/migration.mdx
Evidently Reports and Tests unified,"The guide explains the new Test Suite mode, which integrates test results into the same Report, improving usability and eliminating duplication.",advanced,code,faq/migration.mdx
Evidently library features overview,"The Evidently library offers various data evaluations, reports, and test suites, being suitable for data scientists and AI engineers in Python environments.",beginner,text,faq/oss_vs_cloud.mdx
Cloud vs open-source Evidently features,"The article outlines the core and premium features available in both the open-source and cloud versions of the Evidently platform, highlighting differences in functionality and support.",beginner,text,faq/oss_vs_cloud.mdx
Tracely library open-source details,Tracely is an open-source Python library that captures real-time data from AI applications and integrates with OpenTelemetry.,beginner,text,faq/oss_vs_cloud.mdx
Evidently Enterprise vs Cloud features,"The article compares Evidently Cloud and the Enterprise versions, detailing their deployment options and additional features such as role-based access control and premium capabilities.",intermediate,text,faq/oss_vs_cloud.mdx
Deploy Evidently OSS,"To deploy the Evidently OSS version, users need to manage their own hosting environment, including maintenance tasks like backups and updates.",intermediate,code,faq/oss_vs_cloud.mdx
Benefits of Evidently Cloud,"The Evidently Cloud platform is fully managed, offering automatic updates and scalability, making it an efficient choice for teams focusing on AI development.",intermediate,text,faq/oss_vs_cloud.mdx
telemetry data collection evidently,"The article outlines the types of telemetry data collected by Evidently, including environment and service usage data, while ensuring anonymity.",beginner,text,faq/telemetry.mdx
how to disable telemetry in evidently,The article provides steps to disable telemetry by setting the environment variable `DO_NOT_TRACK` to any value.,beginner,code,faq/telemetry.mdx
is evidently telemetry anonymous,"Evidently collects only anonymous usage data, ensuring personal information is not collected.",beginner,text,faq/telemetry.mdx
evidently telemetry data examples,"The article includes specific examples of telemetry data collected during actions like startup, indexing, and listing projects.",intermediate,code,faq/telemetry.mdx
what data does evidently collect,"The article describes the types of data collected by Evidently, including user environment details and service usage actions.",beginner,text,faq/telemetry.mdx
how to enable telemetry evidently,"To enable telemetry, simply unset the environment variable `DO_NOT_TRACK`. The default setting is telemetry enabled.",beginner,code,faq/telemetry.mdx
evidently user id handling telemetry,The user ID collected in telemetry is anonymized to ensure privacy while still allowing for user action tracking.,intermediate,text,faq/telemetry.mdx
importance of telemetry in evidently,"Telemetry helps the developers understand user engagement, which features are popular, and how to prioritize new developments in Evidently.",intermediate,text,faq/telemetry.mdx
evidently source ip collection,"Evidently obscures the exact source IP address by using `jitsu`, an open-source tool for event collection, ensuring user privacy.",advanced,text,faq/telemetry.mdx
evidently event log structure,"The article presents a structured example of telemetry event logs, showcasing the data format and key-value pairs collected during usage.",advanced,code,faq/telemetry.mdx
benefits of using Evidently AI,"Evidently AI provides a flexible, model-agnostic platform with built-in evaluations for reliable AI product development, allowing users to build without limitations.",beginner,text,faq/why_evidently.mdx
Evidently features and evaluations,"Evidently offers over 100 built-in evaluations covering various ML and LLM use cases, making it easy for developers to assess model performance without extensive setup.",beginner,code,faq/why_evidently.mdx
open-source capabilities of Evidently,"As an open-source library with extensive community support, Evidently allows developers to inspect metrics implementations and utilize an intuitive API to enhance their AI workflows.",intermediate,text,faq/why_evidently.mdx
integration options with Evidently platform,"Evidently integrates seamlessly with existing tools, facilitating easy export of metrics and reports, which enhances collaborative workflows among team members.",intermediate,code,faq/why_evidently.mdx
Evidently Python library examples,The article details how Evidently as a Python library helps in evaluating AI systems with over 100 evaluation metrics and provides examples in the Cookbook section for practical implementation.,beginner,code,introduction.mdx
row-level text evaluations,The article provides a comprehensive reference for performing row-level text evaluations and using different descriptors.,beginner,text,metrics/all_descriptors.mdx
code examples for LLM evaluations,"The article includes a section for code examples focused on LLM evaluations, guiding users through the implementation of various descriptors.",beginner,code,metrics/all_descriptors.mdx
ExactMatch descriptor usage,"The ExactMatch descriptor checks for matching contents between two columns and returns True or False, with syntax examples provided in the article.",intermediate,code,metrics/all_descriptors.mdx
how to validate JSON structure,"To validate JSON structure, you can use the IsValidJSON descriptor, which checks if the content is a valid JSON and returns True or False based on validation results.",intermediate,code,metrics/all_descriptors.mdx
measuring text length in Python,"You can measure the length of a text using the TextLength descriptor, which returns the number of symbols present in the text.",beginner,code,metrics/all_descriptors.mdx
parameters for Contains descriptor,"The Contains descriptor requires a list of items and checks if the text contains any or all of those items, with optional parameters for case sensitivity.",intermediate,code,metrics/all_descriptors.mdx
using CustomDescriptor in evaluations,"The CustomDescriptor allows users to implement their own checks for specific columns as a Python function, providing great flexibility in text evaluations.",advanced,code,metrics/all_descriptors.mdx
returns from DoesNotContain descriptor,The DoesNotContain descriptor returns True or False based on whether specified items are absent in the text.,beginner,code,metrics/all_descriptors.mdx
parameter options for JSONSchemaMatch,JSONSchemaMatch includes parameters like expected_schema and options for exact matching and type validation to ensure structured data integrity.,intermediate,code,metrics/all_descriptors.mdx
context relevance with LLM,"The article explains how to use ContextRelevance to check if context chunks are relevant to a given question, employing various methods for calculation.",advanced,code,metrics/all_descriptors.mdx
using ContextQualityLLMEval,"ContextQualityLLMEval evaluates if context provides enough information to answer a question, returning a label or score based on the evaluation.",intermediate,code,metrics/all_descriptors.mdx
syntax validation descriptors,"Descriptors like IsValidPython and IsValidSQL are provided for validating the syntax of Python code and SQL queries, respectively.",intermediate,code,metrics/all_descriptors.mdx
how to check for biased texts,The BiasLLMEval descriptor detects biased texts and returns a score or label indicating whether bias is present.,intermediate,code,metrics/all_descriptors.mdx
measuring percentage of OOV words,"The OOVWordsPercentage descriptor calculates how many out-of-vocabulary words are present in a text, returning a score from 0 to 100.",beginner,code,metrics/all_descriptors.mdx
function of ItemMatch descriptor,ItemMatch checks if any or all specified items are present in the corresponding row and returns True or False accordingly.,intermediate,code,metrics/all_descriptors.mdx
Detecting negative tones in text,"The NegativityLLMEval checks for negative tones within texts and evaluates them against user-defined criteria, offering scores or labels.",intermediate,code,metrics/all_descriptors.mdx
code to check URL presence,"Use the ContainsLink descriptor to verify if a column contains valid URLs, returning True or False for each row.",beginner,code,metrics/all_descriptors.mdx
parameters for CompletenessLLMEval,CompletenessLLMEval requires a context parameter and evaluates if a response fully utilizes the provided context to return a completeness score.,advanced,code,metrics/all_descriptors.mdx
text statistics evaluation,"The document outlines various descriptors for descriptive statistics about text, such as word count and sentence count, to summarize text attributes.",beginner,text,metrics/all_descriptors.mdx
LLM judge parameters,"The article discusses how to specify LLM judge templates and their parameters to tailor evaluations effectively, ensuring flexibility in scoring criteria.",advanced,code,metrics/all_descriptors.mdx
how to use BeginsWith descriptor,"BeginsWith checks if text starts with a specified prefix, with options for case sensitivity as optional parameters.",beginner,code,metrics/all_descriptors.mdx
how to implement a CustomColumnsDescriptor,"CustomColumnsDescriptor allows for applying custom checks as a Python function, specifically designed to handle any column in your dataset.",advanced,code,metrics/all_descriptors.mdx
determine text sentiment,"The Sentiment descriptor analyzes sentiment in text using a word-based model from NLTK, providing a score ranging from negative to positive.",intermediate,code,metrics/all_descriptors.mdx
checking for PII in text,The PIILLMEval descriptor specifically detects any personally identifiable information within the text and returns appropriate labels.,intermediate,code,metrics/all_descriptors.mdx
evidently metrics reference,"The article serves as a detailed reference for various dataset-level evaluation metrics in machine learning, outlining their functions and usage.",beginner,text,metrics/all_metrics.mdx
how to use TextEvals in evidently,TextEvals() is a large preset that summarizes text evaluation results by displaying ValueStats for all descriptors in a dataset.,beginner,code,metrics/all_metrics.mdx
parameters for ValueStats in evidently,"ValueStats() requires a 'column' parameter and computes various descriptive statistics for that column, including unique and missing value counts.",beginner,code,metrics/all_metrics.mdx
data quality metrics overview,"The article outlines various data quality metrics that check for missing values, out-of-range values, and unique value counts at the column level.",intermediate,text,metrics/all_metrics.mdx
metric cookbook example,Users can reference the Metric cookbook for concrete code examples illustrating how to implement different metrics in their evaluations.,beginner,code,metrics/all_metrics.mdx
understanding test defaults in metrics,"Test defaults in metrics determine conditions that apply during evaluation, which may vary based on whether a reference dataset is provided.",intermediate,text,metrics/all_metrics.mdx
how to calculate precision in evidently,"Precision() calculates the proportion of true positive results among all positive identifications in a dataset, and requires specific visualizations to be set.",intermediate,code,metrics/all_metrics.mdx
evidently ColumnCount metric,"ColumnCount() counts the total number of columns in a dataset, with an optional parameter for test conditions.",beginner,code,metrics/all_metrics.mdx
meaning of dummy model metrics,"Dummy model metrics provide a baseline for model evaluation, comparing metrics such as precision and recall against a random or constant model.",intermediate,text,metrics/all_metrics.mdx
how to check for missing values in evidently,MissingValueCount() counts and reports the number and share of missing values for specified columns in a dataset.,beginner,code,metrics/all_metrics.mdx
data drift detection methods,"The article describes various metrics used to detect data distribution drift for both text and tabular data, including details on calculation methods.",intermediate,text,metrics/all_metrics.mdx
mean error calculation in regression metrics,MeanError() computes the mean error and requires at least one visualization parameter to be set for effective output.,intermediate,code,metrics/all_metrics.mdx
usage of RecallTopK metric,"RecallTopK() calculates the recall for the top K retrieved items to evaluate recommendation systems, while requiring specific parameters.",intermediate,code,metrics/all_metrics.mdx
importance of dataset stats,"Dataset stats provide essential insights into column types, row counts, and data quality, essential for exploratory data analysis.",beginner,text,metrics/all_metrics.mdx
parameters for StandardDeviation in ValueStats,StandardDeviation is one of the metrics computed by ValueStats() and provides insights into the variance within a specified numerical column.,intermediate,code,metrics/all_metrics.mdx
exploratory data analysis metrics,"The article outlines metrics applicable for exploratory data analysis, including those for checking data quality and distribution characteristics.",intermediate,text,metrics/all_metrics.mdx
how to calculate percentage error with MAPE,"MAPE() calculates the Mean Absolute Percentage Error, focusing on forecasting accuracy against median values in regression tasks.",advanced,code,metrics/all_metrics.mdx
what is accuracy metric in classification,Accuracy() evaluates the overall correctness of predictions by calculating the portion of true results from total predictions made.,beginner,text,metrics/all_metrics.mdx
comparing model performance with dummy models,"Dummy models serve as a baseline for comparison, ensuring that trained models outperform these simple metrics as a part of evaluation.",intermediate,text,metrics/all_metrics.mdx
evidently regression metrics overview,"The article provides an overview of various regression metrics like RMSE, MAE, and R that are crucial for assessing model performance.",beginner,text,metrics/all_metrics.mdx
how to check outliers with OutRangeValueCount,"OutRangeValueCount() counts and reports the number and share of values that fall outside specified limits, useful in data validation.",intermediate,code,metrics/all_metrics.mdx
how tests work in evidently,"Tests in evidently allow users to implement conditional checks during metric evaluations using operators like eq, gt, etc., for flexibility.",advanced,text,metrics/all_metrics.mdx
importance of dummy model evaluation metrics,"These metrics help track whether a new model performs better than a naive approach, serving a critical role in performance validation.",intermediate,text,metrics/all_metrics.mdx
settings for DriftedColumnsCount,"DriftedColumnsCount() determines the number of columns that show significant drift compared to a reference, indicating data stability.",advanced,code,metrics/all_metrics.mdx
visualizing classification metrics results,"Classification metrics can be visualized through confusion matrices and PR curves, enhancing interpretability and analysis.",intermediate,text,metrics/all_metrics.mdx
what is a category count metric,"CategoryCount() counts occurrences of specified categories within a column, useful for understanding categorical distributions.",beginner,code,metrics/all_metrics.mdx
how to use QuantileValue in metrics,"QuantileValue() computes the quantile of a specified numerical column, applying a default of 0.5 if no specific quantile is provided.",intermediate,code,metrics/all_metrics.mdx
dataset data quality metrics overview,"Dataset-level data quality metrics assess overall data integrity, including constant columns and duplicate row counts.",beginner,text,metrics/all_metrics.mdx
calculate F1 score with F1Score,"F1Score() calculates the harmonic mean of precision and recall, providing a single metric for evaluating a classification task's accuracy.",intermediate,code,metrics/all_metrics.mdx
how to set parameters for InRangeValueCount,"InRangeValueCount() counts the number of values falling between specified thresholds, helping with range validations in datasets.",intermediate,code,metrics/all_metrics.mdx
evidently metrics for recommender systems,Metrics such as PrecisionTopK and RecallTopK are specified for evaluating the effectiveness of recommendation algorithms in finding relevant items.,advanced,code,metrics/all_metrics.mdx
summarize regression metrics explained,"The article details metrics like R and Mean Absolute Error, outlining their calculation methods and significance in evaluating regression models.",intermediate,text,metrics/all_metrics.mdx
evidently Dummy model quality metrics explained,"Dummy model metrics provide a baseline for model quality by using simple, heuristic calculations for comparison.",beginner,text,metrics/all_metrics.mdx
how to assess data quality with ColumnsWithMissingValuesCount,"ColumnsWithMissingValuesCount() quantifies the number of columns containing missing data, crucial for data cleaning processes.",beginner,code,metrics/all_metrics.mdx
how to use Ranking metrics in evidently,"Ranking metrics evaluate the effectiveness of recommendations and retrieval systems, focusing on metrics like NDCG and MAP.",advanced,code,metrics/all_metrics.mdx
overview of presets for evaluation,The article outlines various pre-built evaluation templates (Presets) that simplify the evaluation process for datasets without requiring setup.,beginner,text,metrics/all_presets.mdx
data drift detection methods,The article outlines various data drift detection methods and parameters to customize detection based on the column type and data characteristics.,beginner,text,metrics/customize_data_drift.mdx
customizing data drift parameters,"You can customize data drift detection by passing specific parameters to metrics or presets, such as drift methods, thresholds, and column shares.",beginner,code,metrics/customize_data_drift.mdx
how to set drift thresholds,It is possible to set thresholds for each drift detection method to enhance the specificity of drift detection in your dataset.,beginner,code,metrics/customize_data_drift.mdx
method for categorical data drift,"The article explains how to choose drift detection methods for categorical columns, such as using the Chi-Square test for categorical data.",intermediate,text,metrics/customize_data_drift.mdx
how to implement custom drift method,You can implement a custom drift detection method by defining a StatTest function and registering it with the required parameters.,advanced,code,metrics/customize_data_drift.mdx
default data drift algorithm,The default data drift algorithm automatically selects the detection method based on the column type and volume of data.,beginner,text,metrics/customize_data_drift.mdx
drift share parameter example,"To set the drift share parameter, use the DataDriftPreset method with a specified drift share, e.g., 0.7 for detecting dataset drift.",intermediate,code,metrics/customize_data_drift.mdx
PSI method for drift detection,You can apply the Population Stability Index (PSI) method for detecting drift across all columns in the dataset by specifying it in the report.,intermediate,code,metrics/customize_data_drift.mdx
detecting drift in numerical columns,"For numerical columns, several statistical tests like K-S and Wasserstein distance are available for effectively detecting data drift.",intermediate,text,metrics/customize_data_drift.mdx
text data drift detection methods,The article describes specific methods for detecting drift in raw text data using statistical hypothesis testing techniques.,intermediate,text,metrics/customize_data_drift.mdx
automated drift detection adjustments,You can override automated data drift detection settings by specifying custom algorithms or thresholds tailored to your dataset.,beginner,code,metrics/customize_data_drift.mdx
analyzing dataset drift,"To analyze dataset drift, you need to assess the share of drifting columns in your metrics, which can be set as a parameter in the report.",intermediate,code,metrics/customize_data_drift.mdx
supported statistical tests for drift detection,"The article lists various supported statistical tests for drift detection, categorizing them based on numerical or categorical data types.",advanced,text,metrics/customize_data_drift.mdx
role of thresholds in drift detection,"Thresholds in drift detection can vary in interpretation; some methods treat higher values as better, while others may mean worse results, depending on the context.",intermediate,text,metrics/customize_data_drift.mdx
how to use drift detection in python,Example code is provided to demonstrate how to use various drift detection methods in Python using the Evidently library.,beginner,code,metrics/customize_data_drift.mdx
using ValueDrift metric,The ValueDrift metric allows for specifying drift detection methods and thresholds for particular columns individually in the dataset.,intermediate,code,metrics/customize_data_drift.mdx
drift detection for text descriptors,You can check for distribution drift in text descriptors by first computing the relevant text descriptors and then applying standard drift detection methods.,intermediate,code,metrics/customize_data_drift.mdx
custom text evaluator example python,"The article provides Python code examples for creating custom text evaluators, such as a custom descriptor for checking if a column is empty or performing exact matches.",beginner,code,metrics/customize_descriptor.mdx
how to use CustomColumnDescriptor Evidently,"Instructions are included on how to implement and use CustomColumnDescriptor in your dataset evaluations, along with example code demonstrating its use.",intermediate,text,metrics/customize_descriptor.mdx
advanced custom descriptor features Evidently,"The article delves into the advanced capabilities of CustomDescriptor, including multi-column checks and returning multiple scores from a dataset.",advanced,text,metrics/customize_descriptor.mdx
using HuggingFace for text evaluation,"The article details the process of employing HuggingFace models to evaluate and classify text data according to various criteria, such as emotions.",beginner,text,metrics/customize_hf_descriptor.mdx
HuggingFace descriptors for emotion classification,"It discusses using specific descriptors from HuggingFace to classify texts based on emotions, providing code examples for implementation.",beginner,code,metrics/customize_hf_descriptor.mdx
install HuggingFace model dependencies,The article assumes familiarity with Python and the necessary imports to work with HuggingFace models in your projects.,beginner,text,metrics/customize_hf_descriptor.mdx
example of using HuggingFace for text scoring,"An example is provided that demonstrates how to score and classify text using the HuggingFace descriptors, including sample code.",intermediate,code,metrics/customize_hf_descriptor.mdx
HuggingFace add_descriptors usage,You will learn how to add built-in evaluators using the `add_descriptors` method to evaluate text data with HuggingFace.,intermediate,code,metrics/customize_hf_descriptor.mdx
custom evaluations HuggingFace,The article explains how to create custom checks by defining your own Python functions for evaluating texts with HuggingFace models.,intermediate,code,metrics/customize_hf_descriptor.mdx
overview of HuggingFace models,"It outlines different types of HuggingFace models available for text classification, including emotion and zero-shot classification models.",beginner,text,metrics/customize_hf_descriptor.mdx
HuggingFace parameters for emotion model,"The article provides detailed parameters needed to specify when using the HuggingFace model for emotion classification, such as the label to evaluate.",advanced,code,metrics/customize_hf_descriptor.mdx
sample data for evaluating HuggingFace,"A toy dataset example is provided in the article to run your evaluations with HuggingFace models, showing how to create a Dataset object.",beginner,code,metrics/customize_hf_descriptor.mdx
integrating HuggingFace with evidently library,Learn how to integrate HuggingFace models into your text evaluation processes using the Evidently library as described in the article.,intermediate,code,metrics/customize_hf_descriptor.mdx
how to use LLM evaluators,The article explains how to utilize built-in LLM evaluators or configure custom criteria by using templates in Python.,beginner,text,metrics/customize_llm_judge.mdx
set up OpenAI API key for LLM,"To use the LLM-based descriptors, you need to set your OpenAI API key as an environment variable as shown in the article.",beginner,code,metrics/customize_llm_judge.mdx
write a binary classification prompt template,The article provides an example of creating a binary classification prompt template for evaluating text characteristics.,intermediate,code,metrics/customize_llm_judge.mdx
evaluate text for toxicity automatically,You can run built-in ToxicityLLMEval on the response column to check for toxic content as demonstrated in the article.,beginner,code,metrics/customize_llm_judge.mdx
implement a custom LLM judge,The article outlines steps to create a custom LLM evaluator by choosing templates and setting evaluation criteria.,intermediate,code,metrics/customize_llm_judge.mdx
configure multi-class classification using LLM,You can use the multi-class classification prompt template to evaluate responses against multiple categories in your LLM integration.,intermediate,code,metrics/customize_llm_judge.mdx
pass additional columns to LLM evaluation,The article explains how to include additional columns in LLM evaluations using placeholders to map them in the criteria.,intermediate,code,metrics/customize_llm_judge.mdx
run single column evaluation with LLM,"To evaluate a single column like 'response' for toxicity, the article provides code examples to add descriptors effectively.",beginner,code,metrics/customize_llm_judge.mdx
how to parameterize LLM evaluators,The article details how you can switch output formats and include reasoning or scores when using LLM evaluators.,intermediate,text,metrics/customize_llm_judge.mdx
use ToxicityLLMEval in code,"You can implement ToxicityLLMEval within Python code to assess the toxicity of provided text data, as shown in the article.",beginner,code,metrics/customize_llm_judge.mdx
Evidently library for text evaluation,The article discusses utilizing the Evidently library for setting up prompt-based evaluators with LLMs.,beginner,text,metrics/customize_llm_judge.mdx
how to change LLM model,You can change the model used in evaluations by specifying the model name during the descriptor setup in your code.,intermediate,code,metrics/customize_llm_judge.mdx
create a custom prompt for evaluation,The article provides guidance on crafting custom prompts based on templates to tailor LLM evaluations to your needs.,advanced,code,metrics/customize_llm_judge.mdx
using models from different providers in LLM,You can select models from various providers by passing the correct provider name and model name parameters as explained in the article.,advanced,code,metrics/customize_llm_judge.mdx
steps to evaluate context quality in text,"You can evaluate context quality by running ContextQualityLLMEval with the appropriate columns, as demonstrated with example code.",intermediate,code,metrics/customize_llm_judge.mdx
understanding LLM-based descriptors in Evidently,The article explains the purpose and functionality of LLM-based descriptors within the Evidently library for evaluating text data.,beginner,text,metrics/customize_llm_judge.mdx
set context for custom LLM evaluations,"To set context for LLM evaluations, update the prompt using pre-messages or specific criteria, as described in the article.",intermediate,code,metrics/customize_llm_judge.mdx
Python imports for LLM evaluation,"The article lists the necessary imports for setting up LLM evaluations in Python, including specific templates and descriptors.",beginner,code,metrics/customize_llm_judge.mdx
how to pass options to LLM providers,The article explains how to use options to directly pass API keys and other parameters to LLM providers during evaluations.,advanced,code,metrics/customize_llm_judge.mdx
get output from LLM evaluations,"After running evaluations, you can view the results using the `as_dataframe()` method as shown in the code examples.",beginner,code,metrics/customize_llm_judge.mdx
LLM evaluation for text responses,The article shows how to set up evaluations for text responses using binary and multi-class classifiers with LLMs.,intermediate,code,metrics/customize_llm_judge.mdx
customizing LLM evaluation criteria,You can customize the criteria for LLM evaluations by defining specific grading logic and categories based on your requirements.,advanced,code,metrics/customize_llm_judge.mdx
check built-in LLM evaluators in Evidently,The article describes how to access and utilize built-in LLM evaluators available in the Evidently library.,beginner,text,metrics/customize_llm_judge.mdx
create custom metrics in data evaluation,"The article explains how to create custom metrics for column or dataset evaluations, including the necessary implementation steps and options for customization.",beginner,code,metrics/customize_metric.mdx
implement custom evaluation metrics,"It details the process of implementing custom calculation methods and optional test conditions for evaluations, along with visualization options.",intermediate,code,metrics/customize_metric.mdx
using Plotly for custom metric visualizations,"The article outlines how to integrate Plotly for visualizing custom metrics, showing examples of setting up visual outputs.",intermediate,code,metrics/customize_metric.mdx
default tests for custom metrics,"It explains how to define default tests that apply to custom metrics when not overridden by custom conditions, ensuring reliable evaluations.",advanced,text,metrics/customize_metric.mdx
classification metrics overview,"The article provides a comprehensive overview of key classification metrics such as accuracy, precision, and recall, crucial for model performance analysis.",beginner,text,metrics/explainer_classification.mdx
evidently classification model metrics examples,"Evidently offers practical examples of how to compute and visualize classification metrics like precision and recall, helping users understand model performance effectively.",beginner,code,metrics/explainer_classification.mdx
importance of confusion matrix in classification,"The article explains how a confusion matrix helps visualize classification errors and their types, which is vital for improving model outcomes.",intermediate,text,metrics/explainer_classification.mdx
how to interpret ROC curve,The article describes how to interpret ROC curves and their significance in evaluating the trade-offs between true positive and false positive rates.,intermediate,text,metrics/explainer_classification.mdx
precision-recall table for model evaluation,"The precision-recall table presented in the article shows outcomes for different classification thresholds, allowing for detailed model evaluation based on prediction coverage.",intermediate,code,metrics/explainer_classification.mdx
class separation quality visualization,This article discusses how the class separation quality visualization showcases correct and incorrect predictions to help in model threshold optimization.,intermediate,code,metrics/explainer_classification.mdx
interactive visualizations for model performance,"Evidently generates interactive visualizations for model performance analysis, aiding in spotting mistakes and developing improvement strategies.",beginner,text,metrics/explainer_classification.mdx
calculating F1-score details,"The article touches on how to calculate the F1-score, highlighting its importance as a balance between precision and recall in model assessment.",advanced,code,metrics/explainer_classification.mdx
data quality summary widget usage,"The article explains how the summary widget presents an overview of datasets, highlighting missing features and constant values.",beginner,text,metrics/explainer_data_stats.mdx
feature widget statistical summaries,It discusses how the features widget generates visualizations and statistical summaries for different types of features within a dataset.,beginner,code,metrics/explainer_data_stats.mdx
visualizations for categorical vs numerical features,"The article provides examples of visualizations created for both categorical and numerical features, showing how they differ in representation.",intermediate,code,metrics/explainer_data_stats.mdx
understanding feature behavior over time,It details how users can analyze feature behavior over time using additional visualizations available through the features widget.,intermediate,text,metrics/explainer_data_stats.mdx
feature interaction with target variable,The article describes how features are visualized in relation to the target variable to understand their influence on outcomes.,intermediate,code,metrics/explainer_data_stats.mdx
correlation insights between features,It outlines how the correlation widget summarizes pairwise correlations and identifies highly correlated variables in the dataset.,beginner,text,metrics/explainer_data_stats.mdx
heatmaps for correlation analysis,"The article mentions that the correlation widget includes heatmaps to visualize relationships between features, although some functionalities have been removed in newer versions.",advanced,text,metrics/explainer_data_stats.mdx
data drift detection algorithm,"The article explains the default algorithm used by Evidently for detecting data drift, focusing on the distribution changes in individual columns of datasets.",beginner,text,metrics/explainer_drift.mdx
how to detect data drift in datasets,Evidently detects data drift by comparing distributions of reference and current datasets using various statistical tests and methods as outlined in the article.,beginner,code,metrics/explainer_drift.mdx
default methods for data drift,"The article lists default methods for data drift detection based on column types and dataset sizes, including tests like Kolmogorov-Smirnov and chi-squared.",intermediate,text,metrics/explainer_drift.mdx
requirements for datasets in drift detection,"To evaluate data drift, the article specifies that two datasets (current and reference) must be non-empty and meet other criteria to avoid errors in drift calculations.",beginner,text,metrics/explainer_drift.mdx
visualizing data drift distributions,"The article describes how to visualize the distribution of columns in data drift analysis using Evidentlys tools, including mean plots and standard deviations.",intermediate,code,metrics/explainer_drift.mdx
custom thresholds for data drift,"It explains how users can set custom thresholds for data drift detection methods in Evidently, adapting the default algorithms as needed.",advanced,code,metrics/explainer_drift.mdx
impact of empty values on drift detection,The article notes that columns with empty values can lead to errors in drift detection and recommends running separate tests on null shares in the dataset.,intermediate,text,metrics/explainer_drift.mdx
text data drift classification,"The article explains how Evidently uses a domain classifier approach to detect drift in text data, comparing ROC AUC scores to detect significant differences.",advanced,code,metrics/explainer_drift.mdx
Recall at K metric definition,"Recall at K measures how many relevant items are present in the top K results of a recommendation system, indicating the system's ability to retrieve relevant items.",beginner,text,metrics/explainer_recsys.mdx
Calculating precision at K,"Precision at K calculates the proportion of relevant results in the top K recommended items, measuring the quality of suggestions made by the system.",beginner,code,metrics/explainer_recsys.mdx
F Beta score calculation,"The F Beta score combines precision and recall into a single metric, allowing for a balanced evaluation of a ranking system's performance depending on the value of Beta used.",intermediate,code,metrics/explainer_recsys.mdx
Mean Average Precision formula,"Mean Average Precision (MAP) assesses how well a system ranks relevant items, focusing on their positions in the top K results, and averages the precision calculated for each relevant item.",intermediate,code,metrics/explainer_recsys.mdx
Interpreting Mean Average Recall,"Mean Average Recall (MAR) provides insight into how effectively a recommendation system retrieves relevant items across different users, averaging the recall calculations.",intermediate,text,metrics/explainer_recsys.mdx
How to compute Normalized Discounted Cumulative Gain,"NDCG compares the ranking quality of recommended items against an ideal order, helping assess the effectiveness of a ranking algorithm in placing relevant items higher.",intermediate,code,metrics/explainer_recsys.mdx
Understanding Hit Rate,"Hit Rate calculates the percentage of users for whom at least one relevant item appears in the top K suggestions, showcasing overall system effectiveness.",beginner,text,metrics/explainer_recsys.mdx
Mean Reciprocal Rank explained,"Mean Reciprocal Rank (MRR) focuses on the position of the first relevant item in the recommendation list, measuring how closely relevant items appear at the top.",intermediate,text,metrics/explainer_recsys.mdx
Score Distribution entropy calculation,"Score Distribution measures the entropy of predicted scores for recommendations, helping visualize how scores are distributed among items in the top K.",advanced,code,metrics/explainer_recsys.mdx
Differences between precision and recall,"Precision and recall are critical metrics for evaluating recommendation systems, where precision measures the relevance of the suggested items and recall assesses the retrieval of all relevant items.",beginner,text,metrics/explainer_recsys.mdx
mean error and absolute error differences,"The article explains Mean Error (ME), Mean Absolute Error (MAE), and their significance in evaluating model performance.",beginner,text,metrics/explainer_regression.mdx
evidently regression metrics examples,"It provides examples of several regression metrics like MAPE, along with how to calculate and interpret them using Evidently.",beginner,code,metrics/explainer_regression.mdx
how to visualize predicted vs actual in regression,The article showcases methods to create scatter plots for visualizing predicted versus actual values to assess model accuracy.,beginner,code,metrics/explainer_regression.mdx
error distribution of regression model,It describes how to analyze the distribution of errors in regression models to understand performance stability.,intermediate,text,metrics/explainer_regression.mdx
interactivity in regression analysis with evidently,Evidently offers interactive visualizations that help in analyzing model predictions and identifying areas for improvement.,intermediate,code,metrics/explainer_regression.mdx
absolute percentage error time series,The article discusses how to visualize absolute percentage error over time to track prediction accuracy across periods.,intermediate,code,metrics/explainer_regression.mdx
explore error bias among different feature values,It emphasizes checking error bias by investigating feature distributions across groups with extreme errors.,advanced,text,metrics/explainer_regression.mdx
link between feature values and regression errors,"The article explores the relationship between feature values and model errors, suggesting how to analyze them for better predictions.",advanced,code,metrics/explainer_regression.mdx
calculating range% between overestimations and underestimations,It explains the formula and process to calculate the range percentage when evaluating the differences between over and underestimations in predictions.,advanced,code,metrics/explainer_regression.mdx
classification preset report example,The article includes Python examples for setting up a Report using the ClassificationPreset to evaluate performance on classification tasks.,beginner,code,metrics/preset_classification.mdx
metrics for classification quality,"It discusses various metrics like Accuracy, Precision, Recall, and F1-score used to evaluate classification model performance.",intermediate,text,metrics/preset_classification.mdx
customizing classification report tests,The article explains how to customize test conditions and metrics in a classification report to better evaluate model performance against specific criteria.,advanced,code,metrics/preset_classification.mdx
data drift preset overview,"The article gives a comprehensive overview of the Data Drift Preset, detailing its function in evaluating shifts in data distribution between datasets.",beginner,text,metrics/preset_data_drift.mdx
how to detect data drift,It explains how to implement the DataDriftPreset in a report to run data drift evaluations between current and reference datasets with simple code examples.,beginner,code,metrics/preset_data_drift.mdx
customize data drift report,"The article describes various customization options for the Data Drift report, including selecting specific columns, adjusting drift detection parameters, and adding other metrics like data quality checks.",intermediate,code,metrics/preset_data_drift.mdx
data drift test suite example,"It provides an example of using a test suite to add explicit pass/fail tests for each column when evaluating data drift, enhancing the analysis of current versus reference datasets.",intermediate,code,metrics/preset_data_drift.mdx
advanced drift detection methods,"The article mentions various advanced drift detection methods that can be customized, like PSI and K-L divergence, allowing for tailored evaluations of data drift.",advanced,text,metrics/preset_data_drift.mdx
Data Summary Preset examples,The article provides code examples for using the Data Summary Preset to generate reports and visualize dataset statistics.,beginner,code,metrics/preset_data_summary.mdx
how to compare datasets with Data Summary,You can use the Data Summary Preset to compare two datasets side-by-side by passing both to the report run method.,intermediate,code,metrics/preset_data_summary.mdx
data quality tests in Data Summary,"The Data Summary Preset enables auto-generated data quality tests based on either a reference dataset or heuristics, helping to ensure data integrity.",intermediate,text,metrics/preset_data_summary.mdx
basic recommender system setup,The article explains how to set up a basic recommender system using the Report and RecSysPreset classes in the Evidently API to generate top-k recommendations.,beginner,code,metrics/preset_recsys.mdx
metrics evaluation for recsys,"It discusses the various metrics available in RecsysPreset for evaluating recommendation quality, including metrics like NDCG at K and diversity score.",intermediate,text,metrics/preset_recsys.mdx
customizing report conditions in recsys,The article outlines methods for modifying test conditions and report composition to enhance performance evaluations in recommender systems.,advanced,code,metrics/preset_recsys.mdx
using RegressionPreset in Python example,The article explains how to create a report using the `RegressionPreset` in Python to evaluate model performance on a dataset.,beginner,code,metrics/preset_regression.mdx
understanding regression quality metrics,"It discusses various regression quality metrics like MAE, RMSE, and how they are used to assess model performance within the report.",intermediate,text,metrics/preset_regression.mdx
compare regression model outputs,The article outlines how to use the `RegressionPreset` to compare performance across different datasets or reference models as part of the evaluation process.,advanced,code,metrics/preset_regression.mdx
running Text Evals report example,"The article provides a code example for running a Text Evals report using the `Report` class in Python, detailing how to evaluate a current dataset.",beginner,code,metrics/preset_text_evals.mdx
data requirements for Text Evals,The article highlights that the Text Evals Preset requires an input dataset with computed descriptors and allows for the comparison of one or two datasets.,intermediate,text,metrics/preset_text_evals.mdx
customizing Text Evals report,"Customization options for the Text Evals report are discussed, including selecting specific descriptors and modifying test conditions to refine the evaluation process.",advanced,code,metrics/preset_text_evals.mdx
evidently lllm evaluation setup,"The article explains how to set up your environment for LLM evaluation using Evidently, including installation instructions and necessary imports.",beginner,code,quickstart_llm.mdx
how to create a report in evidently,"It details the steps to create and run a report summarizing evaluation results in Evidently, including examples of generating reports in JSON or HTML formats.",intermediate,code,quickstart_llm.mdx
define dataset for llm evaluation,"The article describes how to prepare a dataset for LLM evaluation, including examples of input and output structures that are compatible with Evidently.",beginner,code,quickstart_llm.mdx
install evidently library,"Instructions are provided for installing the Evidently library in Python, including usage of pip for installation.",beginner,code,quickstart_llm.mdx
understand llm evaluation descriptors,"It offers insights into the various descriptors used in LLM evaluations, such as Sentiment and TextLength, explaining their purposes and how to implement them.",intermediate,text,quickstart_llm.mdx
using openai key in evidently,The article outlines how to set an OpenAI API key as an environment variable to facilitate evaluations using OpenAI models in Evidently.,intermediate,code,quickstart_llm.mdx
steps to analyze llm output,"It walks through the entire process of analyzing LLM outputs, emphasizing the use of various checks and evaluations to ensure quality responses.",beginner,text,quickstart_llm.mdx
custom llm judges implementation,Instructions are provided on how to implement custom LLM judges using built-in templates in Evidently for specific evaluation criteria.,advanced,code,quickstart_llm.mdx
create dashboard for llm evaluations,"The article explains how to create a dashboard in Evidently to track evaluation results over time, customizing panels for better data visualization.",intermediate,code,quickstart_llm.mdx
data drift evaluation Python example,"The article details how to evaluate data drift in Python using Evidently, including setting up datasets and generating reports.",beginner,code,quickstart_ml.mdx
how to install evidently library,"Instructions for installing the Evidently library in Python are provided, including the command to use.",beginner,code,quickstart_ml.mdx
what is data drift in ML,"Data drift refers to changes in the data distribution over time, impacting model performance; the article explains its significance in ML evaluations.",intermediate,text,quickstart_ml.mdx
create project in evidently,"The article guides users on creating a project in Evidently, including the necessary setup steps and code examples.",beginner,code,quickstart_ml.mdx
generate ML reports in Python,"You can generate ML reports in Python using the Evidently library, and the article outlines the process for running evaluations and viewing results.",intermediate,code,quickstart_ml.mdx
Evidently dashboard configuration,The article includes instructions on how to configure dashboards in Evidently for tracking evaluation results over time.,advanced,code,quickstart_ml.mdx
setup tracing LLM app,"The article outlines the steps to set up tracing for an LLM application, including installing necessary libraries and configuring the tracking environment.",beginner,code,quickstart_tracing.mdx
Evidently Cloud integration with tracing,It explains how to integrate Evidently Cloud for viewing and evaluating LLM traces effectively within the application setup.,intermediate,text,quickstart_tracing.mdx
install tracely and evidently in Python,"The article provides installation commands for the necessary libraries, specifically tracely and evidently, for Python users setting up LLM tracing.",beginner,code,quickstart_tracing.mdx
evaluate LLM responses using evidently,"Instructions for evaluating LLM responses are given, including how to add descriptors and generate reports using the collected trace data.",intermediate,code,quickstart_tracing.mdx
tracing edge cases implementation,"For advanced users, the article covers how to capture edge cases during tracing and provides implementation details for specific monitoring needs.",advanced,code,quickstart_tracing.mdx
adversarial testing examples,"The article provides insights on creating adversarial test datasets to expose AI model vulnerabilities, listing several predefined scenarios for testing, such as harmful content and forbidden topics.",beginner,code,synthetic-data/adversarial_data.mdx
how to configure adversarial dataset,"Instructions are included for configuring your own adversarial dataset in the Evidently UI, including selecting test scenarios and customizing input generation.",intermediate,code,synthetic-data/adversarial_data.mdx
generate test inputs for AI system,"The article details how to create synthetic input test cases in the Evidently platform by defining scenarios and generating relevant questions, expanding test coverage for AI systems.",beginner,code,synthetic-data/input_data.mdx
generate synthetic test data example,"The article explains how to generate synthetic test inputs and outputs using Evidently Cloud to evaluate AI systems, providing practical examples and use cases for this process.",beginner,code,synthetic-data/introduction.mdx
generate test dataset for RAG system,"The article outlines steps to create a synthetic test dataset for RAG systems, guiding users through project creation, uploading knowledge bases, and refining generated test cases.",beginner,code,synthetic-data/rag_data.mdx
when to use synthetic data for AI evaluations,"Synthetic data is essential when real data is unavailable or insufficient, particularly for testing edge cases, adversarial inputs, and during the initial phases of AI development.",beginner,text,synthetic-data/why_synthetic.mdx
examples of generating synthetic data for tests,The article discusses how to create synthetic test datasets to enhance evaluations by quickly generating structured cases and filling gaps in data scenarios.,intermediate,code,synthetic-data/why_synthetic.mdx
